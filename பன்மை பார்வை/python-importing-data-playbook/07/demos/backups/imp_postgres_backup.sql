--
-- PostgreSQL database dump
--

-- Dumped from database version 10.5
-- Dumped by pg_dump version 10.5

SET statement_timeout = 0;
SET lock_timeout = 0;
SET idle_in_transaction_session_timeout = 0;
SET client_encoding = 'UTF8';
SET standard_conforming_strings = on;
SELECT pg_catalog.set_config('search_path', '', false);
SET check_function_bodies = false;
SET client_min_messages = warning;
SET row_security = off;

--
-- Name: plpgsql; Type: EXTENSION; Schema: -; Owner: 
--

CREATE EXTENSION IF NOT EXISTS plpgsql WITH SCHEMA pg_catalog;


--
-- Name: EXTENSION plpgsql; Type: COMMENT; Schema: -; Owner: 
--

COMMENT ON EXTENSION plpgsql IS 'PL/pgSQL procedural language';


SET default_tablespace = '';

SET default_with_oids = false;

--
-- Name: posts; Type: TABLE; Schema: public; Owner: xavier
--

CREATE TABLE public.posts (
    index bigint,
    "AcceptedAnswerId" text,
    "AnswerCount" text,
    "Body" text,
    "ClosedDate" text,
    "CommentCount" text,
    "CommunityOwnedDate" double precision,
    "CreationDate" text,
    "FavoriteCount" text,
    "Id" text,
    "LastActivityDate" text,
    "LastEditDate" text,
    "LastEditorDisplayName" double precision,
    "LastEditorUserId" text,
    "OwnerDisplayName" double precision,
    "OwnerUserId" text,
    "ParentId" text,
    "PostTypeId" text,
    "Score" text,
    "Tags" text,
    "Title" text,
    "ViewCount" text
);


ALTER TABLE public.posts OWNER TO xavier;

--
-- Name: tags; Type: TABLE; Schema: public; Owner: xavier
--

CREATE TABLE public.tags (
    index bigint,
    "Count" text,
    "ExcerptPostId" text,
    "Id" text,
    "TagName" text,
    "WikiPostId" text
);


ALTER TABLE public.tags OWNER TO xavier;

--
-- Name: users; Type: TABLE; Schema: public; Owner: xavier
--

CREATE TABLE public.users (
    index bigint,
    "AboutMe" text,
    "AccountId" text,
    "CreationDate" text,
    "DisplayName" text,
    "DownVotes" text,
    "Id" text,
    "LastAccessDate" text,
    "Location" text,
    "ProfileImageUrl" text,
    "Reputation" text,
    "UpVotes" text,
    "Views" text,
    "WebsiteUrl" text
);


ALTER TABLE public.users OWNER TO xavier;

--
-- Data for Name: posts; Type: TABLE DATA; Schema: public; Owner: xavier
--

COPY public.posts (index, "AcceptedAnswerId", "AnswerCount", "Body", "ClosedDate", "CommentCount", "CommunityOwnedDate", "CreationDate", "FavoriteCount", "Id", "LastActivityDate", "LastEditDate", "LastEditorDisplayName", "LastEditorUserId", "OwnerDisplayName", "OwnerUserId", "ParentId", "PostTypeId", "Score", "Tags", "Title", "ViewCount") FROM stdin;
0	\N	1	<p>I've always been interested in machine learning, but I can't figure out one thing about starting out with a simple "Hello World" example - how can I avoid hard-coding behavior?</p>\n\n<p>For example, if I wanted to "teach" a bot how to avoid randomly placed obstacles, I couldn't just use relative motion, because the obstacles move around, but I don't want to hard code, say, distance, because that ruins the whole point of machine learning.</p>\n\n<p>Obviously, randomly generating code would be impractical, so how could I do this?</p>\n	2014-05-14T14:40:25.950	1	\N	2014-05-13T23:58:30.457	1	5	2014-05-14T00:36:31.077	\N	\N	\N	\N	5	\N	1	9	<machine-learning>	How can I do simple machine learning without hard-coding behavior?	448
1	10	3	<p>As a researcher and instructor, I'm looking for open-source books (or similar materials) that provide a relatively thorough overview of data science from an applied perspective. To be clear, I'm especially interested in a thorough overview that provides material suitable for a college-level course, not particular pieces or papers.</p>\n	2014-05-14T08:40:54.950	4	\N	2014-05-14T00:11:06.457	1	7	2014-05-16T13:45:00.237	2014-05-16T13:45:00.237	\N	97	\N	36	\N	1	4	<education><open-source>	What open-source books (or other materials) provide a relatively thorough overview of data science?	388
2	\N	\N	<p>Not sure if this fits the scope of this SE, but here's a stab at an answer anyway.</p>\n\n<p>With all AI approaches you have to decide what it is you're modelling and what kind of uncertainty there is. Once you pick a framework that allows modelling of your situation, you then see which elements are "fixed" and which are flexible. For example, the model may allow you to define your own network structure (or even learn it) with certain constraints. You have to decide whether this flexibility is sufficient for your purposes. Then within a particular network structure, you can learn parameters given a specific training dataset.</p>\n\n<p>You rarely hard-code behavior in AI/ML solutions. It's all about modelling the underlying situation and accommodating different situations by tweaking elements of the model.</p>\n\n<p>In your example, perhaps you might have the robot learn how to detect obstacles (by analyzing elements in the environment), or you might have it keep track of where the obstacles were and which way they were moving.</p>\n	\N	0	\N	2014-05-14T00:36:31.077	\N	9	2014-05-14T00:36:31.077	\N	\N	\N	\N	51	5	2	5	\N	\N	\N
3	\N	\N	<p>One book that's freely available is "The Elements of Statistical Learning" by Hastie, Tibshirani, and Friedman (published by Springer): <a href="http://statweb.stanford.edu/~tibs/ElemStatLearn/">see Tibshirani's website</a>.</p>\n\n<p>Another fantastic source, although it isn't a book, is Andrew Ng's Machine Learning course on Coursera. This has a much more applied-focus than the above book, and Prof. Ng does a great job of explaining the thinking behind several different machine learning algorithms/situations.</p>\n	\N	1	\N	2014-05-14T00:53:43.273	\N	10	2014-05-14T00:53:43.273	\N	\N	\N	\N	22	7	2	12	\N	\N	\N
4	29	4	<p>I am sure data science as will be discussed in this forum has several synonyms or at least related fields where large data is analyzed.</p>\n\n<p>My particular question is in regards to Data Mining.  I took a graduate class in Data Mining a few years back.  What are the differences between Data Science and Data Mining and in particular what more would I need to look at to become proficient in Data Mining?</p>\n	\N	1	\N	2014-05-14T01:25:59.677	4	14	2014-06-20T17:36:05.023	2014-06-17T16:17:20.473	\N	322	\N	66	\N	1	21	<data-mining><definitions>	Is Data Science the Same as Data Mining?	1243
5	\N	0	<p>In which situations would one system be preferred over the other? What are the relative advantages and disadvantages of relational databases versus non-relational databases?</p>\n	2014-05-14T07:41:49.437	1	\N	2014-05-14T01:41:23.110	\N	15	2014-05-14T01:41:23.110	\N	\N	\N	\N	64	\N	1	2	<databases>	What are the advantages and disadvantages of SQL versus NoSQL in data science?	543
6	46	2	<p>I use <a href="http://www.csie.ntu.edu.tw/~cjlin/libsvm/">Libsvm</a> to train data and predict classification on <strong>semantic analysis</strong> problem. But it has a <strong>performance</strong> issue on large-scale data, because semantic analysis concerns <strong><em>n-dimension</em></strong> problem.</p>\n\n<p>Last year, <a href="http://www.csie.ntu.edu.tw/~cjlin/liblinear/">Liblinear</a> was release, and it can solve performance bottleneck.\nBut it cost too much <strong>memory</strong>. Is <strong>MapReduce</strong> the only way to solve semantic analysis problem on big data? Or are there any other methods that can improve memory bottleneck on <strong>Liblinear</strong>?</p>\n	\N	0	\N	2014-05-14T01:57:56.880	\N	16	2014-05-17T16:24:14.523	2014-05-17T16:24:14.523	\N	84	\N	63	\N	1	18	<machine-learning><bigdata><libsvm>	Use liblinear on big data for semantic analysis	322
7	\N	\N	<p><a href="http://www.csie.ntu.edu.tw/~cjlin/libsvm/" rel="nofollow">LIBSVM</a> is a library for support vector classification (SVM) and regression.\nIt was created by Chih-Chung Chang and Chih-Jen Lin in 2001.</p>\n	\N	0	\N	2014-05-14T02:49:14.580	\N	17	2014-05-16T13:44:53.470	2014-05-16T13:44:53.470	\N	63	\N	63	\N	5	0	\N	\N	\N
8	\N	\N		\N	0	\N	2014-05-14T02:49:14.580	\N	18	2014-05-14T02:49:14.580	2014-05-14T02:49:14.580	\N	-1	\N	-1	\N	4	0	\N	\N	\N
9	37	12	<p>Lots of people use the term <em>big data</em> in a rather <em>commercial</em> way, as a means of indicating that large datasets are involved in the computation, and therefore potential solutions must have good performance. Of course, <em>big data</em> always carry associated terms, like scalability and efficiency, but what exactly defines a problem as a <em>big data</em> problem?</p>\n\n<p>Does the computation have to be related to some set of specific purposes, like data mining/information retrieval, or could an algorithm for general graph problems be labeled <em>big data</em> if the dataset was <em>big enough</em>? Also, how <em>big</em> is <em>big enough</em> (if this is possible to define)?</p>\n	\N	5	\N	2014-05-14T03:56:20.963	20	19	2018-05-01T13:04:43.563	2015-06-11T20:15:28.720	\N	10119	\N	84	\N	1	73	<bigdata><scalability><efficiency><performance>	How big is big data?	7993
10	26	5	<p>we created this social network application for eLearning purposes, it's an experimental thing we are researching on in our lab. it has been used by some case studies for a while and the data on our relational DBMS (SQL Server 2008) is getting big, it's a few gigabytes now and the tables are highly connected to each other. the performance is still fine, but when should we consider other options? is it the matter of performance?  </p>\n	\N	1	\N	2014-05-14T05:37:46.780	1	20	2017-08-29T11:26:37.137	\N	\N	\N	\N	96	\N	1	17	<nosql><relational-dbms>	the data on our relational DBMS is getting big, is it the time to move to NoSQL?	311
17	\N	\N	<p>To answer this question you have to answer which kind of compromise you can afford. RDBMs implements <a href="http://en.wikipedia.org/wiki/ACID">ACID</a>. This is expensive in terms of resources. There are no NoSQL solutions which are ACID. See <a href="http://en.wikipedia.org/wiki/CAP_theorem">CAP theorem</a> to dive deep into these ideas. </p>\n\n<p>So you have to understand each compromise given by each solution and choose the one which is the most appropriate for your problem.</p>\n	\N	0	\N	2014-05-14T07:53:02.560	\N	27	2014-05-14T08:03:37.890	2014-05-14T08:03:37.890	\N	14	\N	108	20	2	10	\N	\N	\N
11	\N	\N	<p>As you rightly note, these days "big data" is something everyone wants to say they've got, which entails a certain looseness in how people define the term.  Generally, though, I'd say you're certainly dealing with big data if the scale is such that it's no longer feasible to manage with more traditional technologies such as RDBMS, at least without complementing them with big data technologies such as Hadoop.</p>\n\n<p>How big your data has to actually be for that to be the case is debatable.  Here's a (somewhat provocative) <a href="http://www.chrisstucchio.com/blog/2013/hadoop_hatred.html">blog post</a> that claims that it's not really the case for less than 5 TB of data.  (To be clear, it doesn't claim "Less than 5 TB isn't big data", but just "Less than 5 TB isn't big enough that you need Hadoop".)</p>\n\n<p>But even on smaller datasets, big data technologies like Hadoop can have other advantages, including being well suited to batch operations, playing well with unstructured data (as well as data whose structure isn't known in advance or could change), horizontal scalability (scaling by adding more nodes instead of beefing up your existing servers), and (as one of the commenters on the above-linked post notes) the ability to integrate your data processing with external data sets (think of a map-reduce where the mapper makes a call to another server).  Other technologies associated with big data, like NoSql databases, emphasize fast performance and consistent availability while dealing with large sets of data, as well also being able to handle semi-unstructured data and to scale horizontally.</p>\n\n<p>Of course, traditional RDBMS have their own advantages including ACID guarantees (Atomicity, Consistency, Isolation, Durability) and better performance for certain operations, as well as being more standardized, more mature, and (for many users) more familiar.  So even for indisputably "big" data, it may make sense to load at least a portion of your data into a traditional SQL database and use that in conjunction with big data technologies.</p>\n\n<p>So, a more generous definition would be that you have big data so long as it's big enough that big data technologies provide some added value for you.  But as you can see, that can depend not just on the size of your data but on how you want to work with it and what sort of requirements you have in terms of flexibility, consistency, and performance.  <em>How</em> you're using your data is more relevant to the question than what you're using it <em>for</em> (e.g. data mining).  That said, uses like data mining and machine learning are more likely to yield useful results if you have a big enough data set to work with.</p>\n	\N	0	\N	2014-05-14T05:44:29.340	\N	21	2014-05-14T05:44:29.340	\N	\N	\N	\N	14	19	2	29	\N	\N	\N
12	24	9	<p>My data set contains a number of numeric attributes and one categorical.</p>\n\n<p>Say, <code>NumericAttr1, NumericAttr2, ..., NumericAttrN, CategoricalAttr</code>, </p>\n\n<p>where <code>CategoricalAttr</code> takes one of three possible values: <code>CategoricalAttrValue1</code>, <code>CategoricalAttrValue2</code> or <code>CategoricalAttrValue3</code>.</p>\n\n<p>I'm using default k-means clustering algorithm implementation for Octave <a href="https://blog.west.uni-koblenz.de/2012-07-14/a-working-k-means-code-for-octave/">https://blog.west.uni-koblenz.de/2012-07-14/a-working-k-means-code-for-octave/</a>.\nIt works with numeric data only.</p>\n\n<p>So my question: is it correct to split the categorical attribute <code>CategoricalAttr</code> into three numeric (binary) variables, like <code>IsCategoricalAttrValue1, IsCategoricalAttrValue2, IsCategoricalAttrValue3</code> ?</p>\n	\N	3	\N	2014-05-14T05:58:21.927	80	22	2018-07-11T11:22:26.170	2014-06-10T07:53:48.253	\N	97	\N	97	\N	1	99	<data-mining><clustering><octave><k-means><categorical-data>	K-Means clustering for mixed numeric and categorical data	105748
13	\N	\N	<p>Data Science specialization from Johns Hopkins University at Coursera would be a great start.\n<a href="https://www.coursera.org/specialization/jhudatascience/1">https://www.coursera.org/specialization/jhudatascience/1</a></p>\n	\N	0	\N	2014-05-14T06:06:13.603	\N	23	2014-05-14T06:06:13.603	\N	\N	\N	\N	97	7	2	8	\N	\N	\N
14	\N	\N	<p>The standard k-means algorithm isn't directly applicable to categorical data, for various reasons.  The sample space for categorical data is discrete, and doesn't have a natural origin.  A Euclidean distance function on such a space isn't really meaningful.  As someone put it, "The fact a snake possesses neither wheels nor legs allows us to say nothing about the relative value of wheels and legs." (from <a href="http://www.daylight.com/meetings/mug04/Bradshaw/why_k-modes.html">here</a>)</p>\n\n<p>There's a variation of k-means known as k-modes, introduced in <a href="http://www.cs.ust.hk/~qyang/Teaching/537/Papers/huang98extensions.pdf">this paper</a> by Zhexue Huang, which is suitable for categorical data.   Note that the solutions you get are sensitive to initial conditions, as discussed <a href="http://arxiv.org/ftp/cs/papers/0603/0603120.pdf">here</a> (PDF), for instance.</p>\n\n<p>Huang's paper (linked above) also has a section on "k-prototypes" which applies to data with a mix of categorical and numeric features.  It uses a distance measure which mixes the Hamming distance for categorical features and the Euclidean distance for numeric features.</p>\n\n<p>A Google search for "k-means mix of categorical data" turns up quite a few more recent papers on various algorithms for k-means-like clustering with a mix of categorical and numeric data.  (I haven't yet read them, so I can't comment on their merits.)  </p>\n\n<hr>\n\n<p>Actually, what you suggest (converting categorical attributes to binary values, and then doing k-means as if these were numeric values) is another approach that has been tried before (predating k-modes).  (See Ralambondrainy, H. 1995. A conceptual version of the k-means algorithm. Pattern Recognition Letters, 16:1147–1157.)  But I believe the k-modes approach is preferred for the reasons I indicated above.</p>\n	\N	9	\N	2014-05-14T06:26:27.163	\N	24	2016-11-29T20:06:51.543	2016-11-29T20:06:51.543	\N	14	\N	14	22	2	89	\N	\N	\N
15	\N	\N	<p>Big Data is defined by the volume of data, that's right, but not only. The particularity of big data is that you need to store a <strong>lots</strong> of <strong>various</strong> and sometimes <strong>unstructured</strong> stuffs <strong>all the times</strong> and from a <strong>tons of sensors</strong>, usually <strong>for years or decade</strong>.</p>\n\n<p>Furthermore you need something scalable, so that it doesn't take you half a year to find a data back.</p>\n\n<p>So here's come Big Data, where traditional method won't work anymore. SQL is not scalable. And SQL works with very structured and linked data (with all those Primary and foreign key mess, innerjoin, imbricated request...). </p>\n\n<p>Basically, because storage becomes cheaper and cheaper and data becomes more and more valuable, big manager ask engineer to records everything. Add to this tons of new sensors with all those mobile, social network, embeded stuff ...etc. So as classic methods won't work, they have to find new technologies (storing everything in files, in json format, with big index, what we call noSQL). </p>\n\n<p>So Big Data may be very big but can be not so big but complexe unstructured or various data which has to be store quickly and on-the-run in a raw format. We focus and storing at first, and then we look at how to link everything together.</p>\n	\N	0	\N	2014-05-14T07:26:04.390	\N	25	2014-05-14T07:26:04.390	\N	\N	\N	\N	104	19	2	7	\N	\N	\N
16	\N	\N	<p>A few gigabytes is not very "<strong>big</strong>". It's more like the normal size of an enterprise DB. As long as you go over PK when joining tables it should work out really well, even in the future (as long as you don't get TB's of data a day).</p>\n\n<p>Most professionals working in a big data environment consider <strong>> ~5TB</strong> as the <em>beginning</em> of the term big data. But even then it's not always the best way to just install the next best nosql database. You should always think about the task that you want to archive with the data (aggregate,read,search,mine,..) to find the best tools for you problem.</p>\n\n<p>i.e. if you do alot of searches in you database it would probably be better to run a solr instance/cluster and denormalize your data from a DBMS like Postgres or your SQL Server from time to time and put it into solr instead of just moving the data from sql to nosql in term of persistence and performance.</p>\n	\N	0	\N	2014-05-14T07:38:31.103	\N	26	2014-05-14T11:03:51.577	2014-05-14T11:03:51.577	\N	115	\N	115	20	2	14	\N	\N	\N
18	\N	\N	<p>There is free ebook "<a href="http://jsresearch.net/" rel="noreferrer">Introduction to Data Science</a>" based on <a href="/questions/tagged/r" class="post-tag" title="show questions tagged &#39;r&#39;" rel="tag">r</a> language</p>\n	\N	0	\N	2014-05-14T07:55:40.133	\N	28	2014-05-14T07:55:40.133	\N	\N	\N	\N	118	7	2	6	\N	\N	\N
19	\N	\N	<p><a href="https://datascience.stackexchange.com/users/36/statsrus">@statsRus</a> starts to lay the groundwork for your answer in another question <a href="https://datascience.stackexchange.com/questions/1/what-characterises-the-difference-between-data-science-and-statistics">https://datascience.stackexchange.com/questions/1/what-characterises-the-difference-between-data-science-and-statistics</a>:</p>\n\n<blockquote>\n  <ul>\n  <li><strong>Data collection</strong>: web scraping and online surveys</li>\n  <li><strong>Data manipulation</strong>: recoding messy data and extracting meaning from linguistic and social network data</li>\n  <li><strong>Data scale</strong>: working with extremely large data sets</li>\n  <li><strong>Data mining</strong>: finding patterns in large, complex data sets, with an emphasis on algorithmic techniques</li>\n  <li><strong>Data communication</strong>: helping turn "machine-readable" data into "human-readable" information via visualization</li>\n  </ul>\n</blockquote>\n\n<h2>Definition</h2>\n\n<p><a href="/questions/tagged/data-mining" class="post-tag" title="show questions tagged &#39;data-mining&#39;" rel="tag">data-mining</a> can be seen as one item (or set of skills and applications) in the toolkit of the data scientist.  I like how he separates the definition of mining from collection in a sort of trade-specific jargon.</p>\n\n<p>However, I think that <em>data-mining</em> would be synonymous with <em>data-collection</em> in a US-English colloquial definition.</p>\n\n<p><em>As to where to go to become proficient?</em>  I think that question is too broad as it is currently stated and would receive answers that are primarily opinion based.  Perhaps if you could refine your question, it might be easier to see what you are asking.</p>\n	\N	0	\N	2014-05-14T07:56:34.437	\N	29	2014-05-14T07:56:34.437	2017-04-13T12:50:41.230	\N	-1	\N	53	14	2	24	\N	\N	\N
20	\N	\N	<p>Total amount of data in the world: 2.8 zetabytes in 2012, estimated to reach 8 zetabytes by 2015 (<a href="http://siliconangle.com/blog/2012/05/21/when-will-the-world-reach-8-zetabytes-of-stored-data-infographic/">source</a>) and with a doubling time of 40 months. Can't get bigger than that :)</p>\n\n<p>As an example of a single large organization, Facebook pulls in 500 terabytes per day, into a 100 petabyte warehouse, and runs 70k queries per day on it as of 2012 (<a href="http://gigaom.com/2012/08/22/facebook-is-collecting-your-data-500-terabytes-a-day/">source</a>)  Their current warehouse is >300 petabytes.</p>\n\n<p>Big data is probably something that is a good fraction of the Facebook numbers (1/100 probably yes, 1/10000 probably not: it's a spectrum not a single number).</p>\n\n<p>In addition to size, some of the features that make it "big" are:</p>\n\n<ul>\n<li><p>it is actively analyzed, not just stored  (quote "If you aren’t taking advantage of big data, then you don’t have big data, you have just a pile of data" Jay Parikh @ Facebook)</p></li>\n<li><p>building and running a data warehouse is a major infrastructure project</p></li>\n<li><p>it is growing at a significant rate</p></li>\n<li><p>it is unstructured or has irregular structure</p></li>\n</ul>\n\n<p>Gartner definition: "Big data is high volume, high velocity, and/or high variety information assets that require new forms of processing" (The 3Vs)  So they also think "bigness" isn't entirely about the size of the dataset, but also about the velocity and structure and the kind of tools needed.</p>\n	\N	3	\N	2014-05-14T08:03:28.117	\N	30	2014-05-14T18:30:59.180	2014-05-14T18:30:59.180	\N	26	\N	26	19	2	20	\N	\N	\N
21	72	1	<p>I have a bunch of customer profiles stored in a <a href="/questions/tagged/elasticsearch" class="post-tag" title="show questions tagged &#39;elasticsearch&#39;" rel="tag">elasticsearch</a> cluster. These profiles are now used for creation of target groups for our email subscriptions. </p>\n\n<p>Target groups are now formed manually using elasticsearch faceted search capabilities (like  get all male customers of age 23 with one car and 3 children).</p>\n\n<p>How could I search for interesting groups <strong>automatically</strong> - using data science, machine learning, clustering or something else?</p>\n\n<p><a href="/questions/tagged/r" class="post-tag" title="show questions tagged &#39;r&#39;" rel="tag">r</a> programming language seems to be a good tool for this task, but I can't form a methodology of such group search. One solution is to somehow find the largest clusters of customers and use them as target groups, so the question is:</p>\n\n<p><strong>How can I automatically choose largest clusters of similar customers (similar by parameters that I don't know at this moment)?</strong></p>\n\n<p>For example: my program will connect to elasticsearch, offload customer data to CSV and using R language script will find that large portion of customers are male with no children and another large portion of customers have a car and their eye color is brown.</p>\n	\N	4	\N	2014-05-14T08:38:07.007	1	31	2014-05-15T05:49:39.140	2014-05-15T05:49:39.140	\N	24	\N	118	\N	1	9	<data-mining><clustering>	Clustering customer data stored in ElasticSearch	1190
22	\N	\N	<p>Is it the time to move to NoSQL will depends on 2 things: </p>\n\n<ol>\n<li>The nature/structure of your data</li>\n<li>Your current performance</li>\n</ol>\n\n<p>SQL databases excel when the data is well structured (e.g. when it can be modeled as a table, an Excel spreadsheet, or a set of rows with a fixed number of columns). Also good when you need to do a lot of table joins (which it sounds like you do).</p>\n\n<p>NoSQL databases excel when the data is un-structured beyond key-value pairs.</p>\n\n<p>Performance wise, you gotta ask yourself one question: <strong>is your current SQL solution slow</strong>? </p>\n\n<p>If not, go with the "<a href="https://en.wikipedia.org/wiki/Bert_Lance#.22If_it_ain.27t_broke.2C_don.27t_fix_it..22" rel="nofollow noreferrer">IIABDFI</a>" principle. </p>\n	\N	0	\N	2014-05-14T09:34:15.477	\N	33	2017-08-29T11:26:37.137	2017-08-29T11:26:37.137	\N	132	\N	132	20	2	6	\N	\N	\N
23	\N	3	<p>In working on exploratory data analysis, and developing algorithms, I find that most of my time is spent in a cycle of visualize, write some code, run on small dataset, repeat.   The data I have tends to be computer vision/sensor fusion type stuff, and algorithms are vision-heavy (for example object detection and tracking, etc), and the off the shelf algorithms don't work in this context.  I find that this takes a lot of iterations (for example, to dial in the type of algorithm or tune the parameters in the algorithm, or to get a visualization right) and also the run times even on a small dataset are quite long, so all together it takes a while. </p>\n\n<p>How can the algorithm development itself be sped up and made more scalable?</p>\n\n<p>Some specific challenges: </p>\n\n<p>How can the number of iterations be reduced?  (Esp. when what kind of algorithm, let alone the specifics of it, does not seem to be easily foreseeable without trying different versions and examining their behavior)</p>\n\n<p>How to run on bigger datasets during development?  (Often going from small to large dataset is when a bunch of new behavior and new issues is seen)</p>\n\n<p>How can algorithm parameters be tuned faster?</p>\n\n<p>How to apply machine learning type tools to algorithm development itself?  (For example, instead of writing the algorithm by hand, write some simple building blocks and combine them in a way learned from the problem, etc)</p>\n	\N	0	\N	2014-05-14T09:51:54.753	3	35	2014-05-20T03:56:43.147	\N	\N	\N	\N	26	\N	1	18	<algorithms>	How to scale up algorithm development?	346
24	\N	\N	<p>To me (coming from a relational database background), "Big Data" is not primarily about the data size (which is the bulk of what the other answers are so far).</p>\n\n<p>"Big Data" and "Bad Data" are closely related. Relational Databases require 'pristine data'. If the data is in the database, it is accurate, clean, and 100% reliable. Relational Databases require "Great Data"  and a huge amount of time, money, and accountability is put on to making sure the data is well prepared before loading it in to the database. If the data is in the database, it is 'gospel', and it defines the system understanding of reality.</p>\n\n<p>"Big Data" tackles this problem from the other direction. The data is poorly defined, much of it may be inaccurate, and much of it may in fact be missing. The structure and layout of the data is linear as opposed to relational.</p>\n\n<p>Big Data has to have enough volume so that the amount of bad data, or missing data becomes statistically insignificant. When the errors in your data are common enough to cancel each other out, when the missing data is proportionally small enough to be negligible and when your data access requirements and algorithms are functional even with incomplete and inaccurate data, then you have "Big Data".</p>\n\n<p>"Big Data" is not really about the volume, it is about the characteristics of the data.</p>\n	\N	4	\N	2014-05-14T10:41:23.823	\N	37	2018-05-01T13:04:43.563	2018-05-01T13:04:43.563	\N	51450	\N	9	19	2	77	\N	\N	\N
25	43	2	<p>I heard about many tools / frameworks for helping people to process their data (big data environment). </p>\n\n<p>One is called Hadoop and the other is the noSQL concept. What is the difference in point of processing? </p>\n\n<p>Are they complementary?</p>\n	\N	2	\N	2014-05-14T10:44:58.933	2	38	2015-05-18T12:30:19.497	2014-05-14T22:26:59.453	\N	134	\N	134	\N	1	14	<nosql><tools><processing><apache-hadoop>	What is the difference between Hadoop and noSQL	2985
26	\N	\N	<p>Big Data is actually not so about the "how big it is". </p>\n\n<p>First, few gigabytes is not big at all, it's almost nothing. So don't bother yourself, your system will continu to work efficiently for some time I think.</p>\n\n<p>Then you have to think of how do you use your data. </p>\n\n<ul>\n<li>SQL approach: Every data is precious, well collected and selected, and the focus is put on storing high valuable and well structured data. This can be costly, everything is interlink, and it's good for well stuctured system and functionnal data.</li>\n<li>Big Data approach: In big data you basically store almost everything, regardless of the value it has, and then do a active analytics process. Things are not linked, they are copied. For example let's say I have a blog entry. In Big Data there will not be a link to its author, but the author will be embedded inside the blog entry. Way more scalable, but require a different and more complex approach.</li>\n</ul>\n\n<p>If your storing "functionnal" data use by your application, I will suggest you to remain on SQL. If your storing data in order to search on them later or to do reporting, and if this amount of data may increase quickly, I will suggest big data.\nIn my opinion, big data is useful when you are dealing with real data that have to be collect and analyzed continuously.</p>\n	\N	0	\N	2014-05-14T11:12:03.880	\N	40	2014-05-14T11:12:03.880	\N	\N	\N	\N	104	20	2	8	\N	\N	\N
27	44	8	<p>R has many libraries which are aimed at Data Analysis (e.g. JAGS, BUGS, ARULES etc..), and is mentioned in popular textbooks such as: J.Krusche, Doing Bayesian Data Analysis; B.Lantz, "Machine Learning with R".</p>\n\n<p>I've seen a guideline of 5TB for a dataset to be considered as Big Data.</p>\n\n<p>My question is: Is R suitable for the amount of Data typically seen in Big Data problems? \nAre there strategies to be employed when using R with this size of dataset?</p>\n	\N	1	\N	2014-05-14T11:15:40.907	18	41	2015-04-12T05:00:23.663	2014-05-14T13:06:28.407	\N	118	\N	136	\N	1	43	<bigdata><r>	Is the R language suitable for Big Data	4571
28	\N	\N	<p>NoSQL is a way to store data that does not require there to be some sort of relation. The simplicity of its design and horizontal scale-ability, one way they store data is the <code>key : value</code> pair design. This lends itself to processing that is similar to Hadoop. The use of a NoSQL db really depends on the type of problem that one is after.</p>\n\n<p>Here is a good wikipedia link <a href="https://en.wikipedia.org/wiki/NoSQL" rel="noreferrer">NoSQL</a></p>\n\n<p>Hadoop is a system that is meant to store and process huge chunks of data. It is a distributed file system dfs. The reason it does this is that central to its design it makes the assumption that hardware failures are common, thus making multiple copies of the same piece of information and spreading it across multiple machines and racks, so if one goes down, no problem, we have two more copies. Here is a great link for Hadoop from wikipedia as well, you will see that it is, in my opinion more than just storage, but also processing:\n <a href="https://en.wikipedia.org/wiki/Apache_Hadoop" rel="noreferrer">Hadoop</a></p>\n	\N	0	\N	2014-05-14T11:21:31.500	\N	42	2014-05-14T11:21:31.500	\N	\N	\N	\N	59	38	2	5	\N	\N	\N
29	\N	\N	<p><strong>Hadoop is not a database</strong>, hadoop is an entire ecosystem.</p>\n\n<p><img src="https://i.stack.imgur.com/oOYp7.png" alt="the hadoop ecosystem"></p>\n\n<p>Most people will refer to <a href="http://de.wikipedia.org/wiki/MapReduce" rel="nofollow noreferrer">mapreduce</a> jobs while talking about hadoop. A mapreduce job splits big datasets in some little chunks of data and spread them over a cluster of nodes to get proceed. In the end the result from each node will be put together again as one dataset.</p>\n\n<hr>\n\n<p>Let's assume you load into hadoop a set of <code>&lt;String, Integer&gt;</code> with the population of some neighborhoods within a city and you want to get the average population over the whole neighborhoods of each city(figure 1).</p>\n\n<p><em>figure 1</em></p>\n\n<pre><code>    [new york, 40394]\n    [new york, 134]\n    [la, 44]\n    [la, 647]\n    ...\n</code></pre>\n\n<p>Now hadoop will first map each value by using the keys (figure 2)</p>\n\n<p><em>figure 2</em></p>\n\n<pre><code>[new york, [40394,134]]\n[la, [44,647]]\n...\n</code></pre>\n\n<p>After the mapping it will reduce the values of each key to a new value (in this example the average over the value set of each key)(figure 3)</p>\n\n<p><em>figure 3</em></p>\n\n<pre><code>[new york, [20264]]\n[la, [346]]\n...\n</code></pre>\n\n<p>now hadoop would be done with everything. You can now load the result into the HDFS (hadoop distributed file system) or into any DBMS or file.</p>\n\n<p>Thats just one <strong>very basic</strong> and <strong>simple</strong> example of what hadoop can do. You can run much more complicated tasks in hadoop.</p>\n\n<p>As you already mentioned in your question, hadoop and noSQL are complementary. I know a few setups where i.e. billions of datasets from sensors are stored in HBase and get then through hadoop to finally be stored in a DBMS.</p>\n	\N	0	\N	2014-05-14T11:23:25.913	\N	43	2015-05-18T12:30:19.497	2015-05-18T12:30:19.497	\N	115	\N	115	38	2	15	\N	\N	\N
30	\N	\N	<p>Actually this is coming around. In the book R in a Nutshell there is even a section on using R with Hadoop for big data processing. There are some work arounds that need to be done because R does all it's work in memory, so you are basically limited to the amount of RAM you have available to you.</p>\n\n<p>A mature project for R and Hadoop is <a href="https://github.com/RevolutionAnalytics/RHadoop">RHadoop</a></p>\n\n<p>RHadoop has been divided into several sub-projects, rhdfs, rhbase, rmr2, plyrmr, and quickcheck (<a href="https://github.com/RevolutionAnalytics/RHadoop/wiki">wiki</a>).</p>\n	\N	3	\N	2014-05-14T11:24:39.530	\N	44	2015-01-31T11:34:03.700	2015-01-31T11:34:03.700	\N	2522	\N	59	41	2	40	\N	\N	\N
31	\N	\N	<p>First off, if your data has as many variations (in function of time, context, and others) as to make it hard to apply a single strategy to cope with it, you may be interested in doing a prior temporal/contextual/... characterization of the dataset. Characterizing data, i.e., extracting information about how the volume or specifics of the content varies according to some criteria, usually provides with a better understanding (more consise and precise) than simply inferring algorithms on a brute-force fashion.</p>\n\n<p>So, answering each question:</p>\n\n<ol>\n<li>characterization is definitely a means of reducing the number of <em>iterations</em> while trying to select proper algorithms for specific data;</li>\n<li>if you have a discrete set of criterias on which your data varies, it becomes much easier to <em>scale up</em> solutions, as will know what information you'd gain/lose if simpler/specific solutions were applied;</li>\n<li>after a characterization, you should be also easier to select parameters, since you'd know what kind of <em>specific data</em> you'd be dealing with;</li>\n<li>finally, you may use data mining/machine learning algorithms to support this characterization. This includes using:\n<ul>\n<li>clustering algorithms, to reduce the dimensionality of data;</li>\n<li>classification algorithms, to help deciding on specific properties the data in function of time/context/... may present;</li>\n<li>association rules, to predict particular knowledge from the dataset, while also improving/fine-graining the data used for later analysis;</li>\n<li>and other possible strategies and analyses.</li>\n</ul></li>\n</ol>\n\n<p>And <a href="http://www.cs.cmu.edu/Groups/sage/sagedc.html">here</a> is a list of some criterias on which to analyse data, which you may find helpful.</p>\n	\N	0	\N	2014-05-14T11:26:40.580	\N	45	2014-05-14T11:33:27.770	2014-05-14T11:33:27.770	\N	84	\N	84	35	2	6	\N	\N	\N
32	\N	\N	<p>Note that there is an early version of LIBLINEAR ported to <a href="http://spark.apache.org">Apache Spark</a>. See <a href="http://apache-spark-user-list.1001560.n3.nabble.com/Spark-LIBLINEAR-td5546.html">mailing list comments</a> for some early details, and the <a href="http://www.csie.ntu.edu.tw/~cjlin/libsvmtools/distributed-liblinear/">project site</a>.</p>\n	\N	3	\N	2014-05-14T12:32:29.503	\N	46	2014-05-14T21:03:05.313	2014-05-14T21:03:05.313	\N	21	\N	21	16	2	11	\N	\N	\N
33	\N	\N	<p>The main problem with using R for large data sets is the RAM constraint. The reason behind keeping all the data in RAM is that it provides much faster access and data manipulations than would storing on HDDs. If you are willing to take a hit on performance, then yes, it is quite practical to work with <a href="http://statistics.org.il/wp-content/uploads/2010/04/Big_Memory%20V0.pdf">large datasets in R</a>.</p>\n\n<ul>\n<li>RODBC Package: Allows connecting to external DB from R to retrieve and handle data. Hence, the data being <em>manipulated</em> is restricted to your RAM. The overall data set can go much larger.</li>\n<li>The ff package allows using larger than RAM data sets by utilising memory-mapped pages.</li>\n<li>BigLM: It builds generalized linear models on big data. It loads data into memory in chunks.</li>\n<li>bigmemory : An R package which allows powerful and memory-efficient parallel \nanalyses and data mining of massive data sets. It permits storing large objects (matrices etc.) in memory (on the RAM) using external pointer objects to refer to them. </li>\n</ul>\n	\N	1	\N	2014-05-14T12:39:41.197	\N	47	2014-05-14T12:39:41.197	\N	\N	\N	\N	62	41	2	30	\N	\N	\N
34	\N	\N	<p><a href="http://www.r-project.org" rel="nofollow">R</a> is a language and environment for statistical computing and graphics. It is a GNU project which is similar to the S language and environment which was developed at Bell Laboratories (formerly AT&amp;T, now Lucent Technologies) by John Chambers and colleagues. R can be considered as a different implementation of S. There are some important differences, but much code written for S runs unaltered under R.</p>\n\n<p>R provides a wide variety of statistical (linear and nonlinear modelling, classical statistical tests, time-series analysis, classification, clustering, ...) and graphical techniques, and is highly extensible. The S language is often the vehicle of choice for research in statistical methodology, and R provides an Open Source route to participation in that activity.</p>\n\n<p>One of R's strengths is the ease with which well-designed publication-quality plots can be produced, including mathematical symbols and formulae where needed. Great care has been taken over the defaults for the minor design choices in graphics, but the user retains full control.</p>\n\n<p>R was created by <a href="http://www.stat.auckland.ac.nz/~ihaka/" rel="nofollow">Ross Ihaka</a> and <a href="http://www.gene.com/scientists/our-scientists/robert-gentleman" rel="nofollow">Robert Gentleman</a> and is now developed by the <a href="http://www.r-project.org/contributors.html" rel="nofollow">R Development Core Team</a>. The R environment is easily extended through a packaging system on <a href="http://cran.r-project.org" rel="nofollow" title="CRAN The Comprehensive R Archive Network">CRAN</a>. </p>\n\n<p>R is available as Free Software under the terms of the Free Software Foundation's GNU General Public License in source code form. It compiles and runs on a wide variety of UNIX platforms and similar systems (including FreeBSD and Linux), Windows and Mac OS.</p>\n	\N	0	\N	2014-05-14T13:08:26.647	\N	48	2014-08-16T17:29:43.517	2014-08-16T17:29:43.517	\N	2961	\N	201	\N	5	0	\N	\N	\N
35	\N	\N	R is a free, open-source programming language and software environment for statistical computing, bioinformatics, and graphics.	\N	0	\N	2014-05-14T13:08:26.647	\N	49	2014-08-15T16:38:27.880	2014-08-15T16:38:27.880	\N	2961	\N	2961	\N	4	0	\N	\N	\N
36	53	2	<p>I have an R script that generates a report based on the current contents of a database. This database is constantly in flux with records being added/deleted many times each day. How can I ask my computer to run this every night at 4 am so that I have an up to date report waiting for me in the morning? Or perhaps I want it to re-run once a certain number of new records have been added to the database. How might I go about automating this? I should mention I'm on Windows, but I could easily put this script on my Linux machine if that would simplify the process. </p>\n	2017-08-29T10:27:14.067	1	\N	2014-05-14T14:26:54.313	\N	50	2014-05-14T15:42:02.393	\N	\N	\N	\N	151	\N	1	6	<r><databases><efficiency><tools>	Running an R script programmatically	490
37	\N	\N	<blockquote>\n  <p>How can I ask my computer to run this every night at 4 am so that I have an up to date report waiting for me in the morning?</p>\n</blockquote>\n\n<p>You can set up a cronjob on a Linux system. These are run at the set time, if the computer is on. To do so, open a terminal and type:</p>\n\n<pre><code>crontab -e\n</code></pre>\n\n<p>And add:</p>\n\n<pre><code>00 4 * * * r source(/home/FilePath/.../myRscript.R)\n</code></pre>\n\n<p>Source: <a href="https://stackoverflow.com/a/14710307/1488917">Stack Overflow</a></p>\n	\N	2	\N	2014-05-14T14:48:32.180	\N	51	2014-05-14T14:57:32.243	2017-05-23T12:38:53.587	\N	-1	\N	62	50	2	9	\N	\N	\N
38	\N	7	<p>From my limited dabbling with data science using R, I realized that cleaning bad data is a very important part of preparing data for analysis. </p>\n\n<p>Are there any best practices or processes for cleaning data before processing it? If so, are there any automated or semi-automated tools which implement some of these best practices?</p>\n	\N	2	\N	2014-05-14T15:25:21.700	18	52	2017-01-23T06:27:10.313	2014-05-16T13:45:07.447	\N	136	\N	157	\N	1	33	<r><data-cleaning>	Organized processes to clean data	2922
39	\N	\N	<p>For windows, use the task scheduler to set the task to run for example daily at 4:00 AM</p>\n\n<p>It gives you many other options regarding frequency etc.\n<a href="http://en.wikipedia.org/wiki/Windows_Task_Scheduler">http://en.wikipedia.org/wiki/Windows_Task_Scheduler</a></p>\n	\N	0	\N	2014-05-14T15:42:02.393	\N	53	2014-05-14T15:42:02.393	\N	\N	\N	\N	116	50	2	10	\N	\N	\N
40	\N	\N	<p>From my point of view, this question is suitable for a two-step answer. The first part, let us call it <em>soft preprocessing</em>, could be taken as the usage of different data mining algorithms to preprocess data in such a way that makes it suitable for further analyses. Notice that this could be the analysis itself, in case the goal is simple enough to be tackled in a single shot.</p>\n\n<p>The second part, the <em>hard preprocessing</em>, actually comes prior to any other process, and is may be taken as the usage of simple tools or scripts to clean up data, selecting specific contents to be processed. To this problem, POSIX provides us with a wonderous set of magic tools, which can be used to compose concise -- and very powerful -- preprocessing scripts.</p>\n\n<p>For example, for people who deal with data coming from social websites (twitter, facebook, ...), the <em>data retrieval</em> usually yields files with very specific format -- although not always nicely structure, as they may contain missing fields, and so. For these cases, a simple <code>awk</code> script could clean up the data, producing a <em>valid</em> input file for later processing. From the magic set, one may also point out <code>grep</code>, <code>sed</code>, <code>cut</code>, <code>join</code>, <code>paste</code>, <code>sort</code>, and a whole multitude of other tools.</p>\n\n<p>In case simple the source file has too many nitty-gritties, it may also be necessary to produce a bundle of methods to clean up data. In such cases, it is usually better to use scripting languages (other than shell ones), such as Python, Ruby, and Perl. This allows for building up <em>API</em>'s to select specific data in a very straightforward and reusable way. Such <em>API</em>'s are sometimes made public by their writers, such as <a href="http://imdbpy.sourceforge.net/">IMDbPY</a>, <a href="https://api.stackexchange.com/docs">Stack Exchange API</a>, and many others.</p>\n\n<p>So, answering the question: are there any best practices? It usually depends on your task. If you will always deal with the same data format, it's commonly best to write an <em>organized</em> script to preprocess it; whereas, if you just need a simple and fast clean up on some dataset, count on POSIX tools for concise shell scripts that will do the whole job <strong>much</strong> faster than a Python script, or so. Since the <em>clean up</em> depends both on the dataset and on your purposes, it's hard to have everything already done. Yet, there are lots of API's that puts you halfway through with the problem.</p>\n	\N	6	\N	2014-05-14T16:29:39.927	\N	57	2014-05-14T16:29:39.927	\N	\N	\N	\N	84	52	2	16	\N	\N	\N
41	\N	\N	<p>You can check out <a href="http://hunch.net/~vw/">vowpal wabbit</a>. It is quite popular for large-scale learning and includes parallel provisions.</p>\n\n<p>From their website:</p>\n\n<blockquote>\n  <p>VW is the essence of speed in machine learning, able to learn from terafeature datasets with ease. Via parallel learning, it can exceed the throughput of any single machine network interface when doing linear learning, a first amongst learning algorithms.</p>\n</blockquote>\n	\N	1	\N	2014-05-14T17:06:33.337	\N	58	2014-05-14T17:06:33.337	\N	\N	\N	\N	119	16	2	11	\N	\N	\N
42	316	3	<p>In reviewing “<a href="http://rads.stackoverflow.com/amzn/click/1461468485" rel="nofollow">Applied Predictive Modeling</a>" a <a href="http://www.information-management.com/blogs/applied-predictive-modeling-10024771-1.html" rel="nofollow">reviewer states</a>:</p>\n\n<blockquote>\n  <p>One critique I have of statistical learning (SL) pedagogy is the\n  absence of computation performance considerations in the evaluation of\n  different modeling techniques. With its emphases on bootstrapping and\n  cross-validation to tune/test models, SL is quite compute-intensive.\n  Add to that the re-sampling that's embedded in techniques like bagging\n  and boosting, and you have the specter of computation hell for\n  supervised learning of large data sets. <strong>In fact, R's memory\n  constraints impose pretty severe limits on the size of models that can\n  be fit by top-performing methods like random forests.</strong> Though SL does a\n  good job calibrating model performance against small data sets, it'd\n  sure be nice to understand performance versus computational cost for\n  larger data.</p>\n</blockquote>\n\n<p>What are R's memory constraints, and do they impose severe limits on the size of models that can be fit by top-performing methods like <a href="http://en.wikipedia.org/wiki/Random_forest" rel="nofollow">random forests</a>?</p>\n	\N	1	\N	2014-05-14T17:48:21.240	\N	59	2014-07-26T15:10:51.000	2014-07-26T15:10:51.000	\N	62	\N	158	\N	1	10	<apache-hadoop><r>	What are R's memory constraints?	964
43	\N	\N	<p>R performs all computation in-memory so you can't perform operation on a dataset that is larger than available RAM amount. However there are some libraries that allow bigdata processing using R and one of popular libraries for bigdata processing like Hadoop.</p>\n	\N	0	\N	2014-05-14T17:58:48.297	\N	60	2014-05-14T17:58:48.297	\N	\N	\N	\N	118	59	2	7	\N	\N	\N
44	62	8	<p>Logic often states that by overfitting a model, its capacity to generalize is limited, though this might only mean that overfitting stops a model from improving after a certain complexity. Does overfitting cause models to become worse regardless of the complexity of data, and if so, why is this the case?</p>\n\n<hr>\n\n<p><strong>Related:</strong> Followup to the question above, "<a href="https://datascience.stackexchange.com/questions/361/when-is-a-model-underfitted">When is a Model Underfitted?</a>"</p>\n	\N	16	\N	2014-05-14T18:09:01.940	17	61	2017-09-17T02:27:31.110	2017-04-13T12:50:41.230	\N	-1	\N	158	\N	1	45	<machine-learning><predictive-modeling>	Why Is Overfitting Bad in Machine Learning?	7809
45	\N	\N	<p>Overfitting is <em>empirically</em> bad.  Suppose you have a data set which you split in two, test and training.  An overfitted model is one that performs much worse on the test dataset than on training dataset.  It is often observed that models like that also in general perform worse on additional (new) test datasets than models which are not overfitted.  </p>\n\n<p>One way to understand that intuitively is that a model may use some relevant parts of the data (signal) and some irrelevant parts (noise).  An overfitted model uses more of the noise, which increases its performance in the case of known noise (training data) and decreases its performance in the case of novel noise (test data).  The difference in performance between training and test data indicates how much noise the model picks up; and picking up noise directly translates into worse performance on test data (including future data).</p>\n\n<p>Summary: overfitting is bad by definition, this has not much to do with either complexity or ability to generalize, but rather has to do with mistaking noise for signal.</p>\n\n<p>P.S. On the "ability to generalize" part of the question, it is very possible to have a model which has inherently limited ability to generalize due to the structure of the model (for example linear SVM, ...) but is still prone to overfitting.  In a sense overfitting is just one way that generalization may fail.</p>\n	\N	0	\N	2014-05-14T18:27:56.043	\N	62	2015-02-12T07:08:27.463	2015-02-12T07:08:27.463	\N	26	\N	26	61	2	43	\N	\N	\N
58	139	1	<p><em>(Note: Pulled this question from the <a href="http://area51.stackexchange.com/proposals/55053/data-science/57398#57398">list of questions in Area51</a>, but believe the question is self explanatory. That said, believe I get the general intent of the question, and as a result likely able to field any questions on the question that might pop-up.)</em> </p>\n\n<p><strong>Which Big Data technology stack is most suitable for processing tweets, extracting/expanding URLs and pushing (only) new links into 3rd party system?</strong></p>\n	\N	6	\N	2014-05-15T00:39:33.433	1	76	2014-05-18T15:18:08.050	2014-05-18T15:18:08.050	\N	118	\N	158	\N	1	6	<bigdata><tools><data-stream-mining>	Which Big Data technology stack is most suitable for processing tweets, extracting/expanding URLs and pushing (only) new links into 3rd party system?	163
59	87	2	<p><strong>Background:</strong> Following is from the book <a href="http://rads.stackoverflow.com/amzn/click/1449356265">Graph Databases</a>, which covers a performance test mentioned in the book <a href="http://rads.stackoverflow.com/amzn/click/1617290769">Neo4j in Action</a>:</p>\n\n<blockquote>\n  <p>Relationships in a graph naturally form paths. Querying, or\n  traversing, the graph involves following paths. Because of the\n  fundamentally path-oriented nature of the datamodel, the majority of\n  path-based graph database operations are highly aligned with the way\n  in which the data is laid out, making them extremely efficient. In\n  their book Neo4j in Action, Partner and Vukotic perform an experiment\n  using a relational store and Neo4j.</p>\n  \n  <p>The comparison shows that the graph database is substantially quicker\n  for connected data than a relational store.Partner and Vukotic’s\n  experiment seeks to find friends-of-friends in a social network, to a\n  maximum depth of five. Given any two persons chosen at random, is\n  there a path that connects them which is at most five relationships\n  long? For a social network containing 1,000,000 people, each with\n  approximately 50 friends, the results strongly suggest that graph\n  databases are the best choice for connected data, as we see in Table\n  2-1.</p>\n  \n  <p>Table 2-1. Finding extended friends in a relational database versus efficient finding in Neo4j</p>\n\n<pre><code>Depth   RDBMS Execution time (s)    Neo4j Execution time (s)     Records returned\n2       0.016                       0.01                         ~2500    \n3       30.267                      0.168                        ~110,000 \n4       1543.505                    1.359                        ~600,000 \n5       Unfinished                  2.132                        ~800,000\n</code></pre>\n  \n  <p>At depth two (friends-of-friends) both the relational database and the graph database perform well enough for us to consider using them in an online system. While the Neo4j query runs in two-thirds the time of the relational one, an end-user would barely notice the the difference in milliseconds between the two. By the time we reach depth three (friend-of-friend-of-friend), however, it’s clear that the relational database can no longer deal with the query in a reasonable timeframe: the thirty seconds it takes to complete would be completely unacceptable for an online system. In contrast, Neo4j’s response time remains relatively flat: just a fraction of a second to perform the query—definitely quick enough for an online system.</p>\n  \n  <p>At depth four the relational database exhibits crippling latency,\n  making it practically useless for an online system. Neo4j’s timings\n  have deteriorated a little too, but the latency here is at the\n  periphery of being acceptable for a responsive online system. Finally,\n  at depth five, the relational database simply takes too long to\n  complete the query. Neo4j, in contrast, returns a result in around two\n  seconds. At depth five, it transpires almost the entire network is our\n  friend: for many real-world use cases, we’d likely trim the results,\n  and the timings.</p>\n</blockquote>\n\n<p><strong>Questions are:</strong> </p>\n\n<ul>\n<li>Is this a reasonable test to emulate what one might except to find in a social network? <em>(Meaning do real social networks normally have nodes with approximately 50 friends for example; seems like the "<a href="http://en.wikipedia.org/wiki/The_rich_get_richer_%28statistics%29">rich get richer</a>" model would be more natural for social networks, though might be wrong.)</em></li>\n<li>Regardless of the naturalness of the emulation, is there any reason to believe the results are off, or unreproducible? </li>\n</ul>\n	\N	0	\N	2014-05-15T01:22:35.167	1	77	2015-05-10T21:18:01.617	2014-05-15T13:15:02.727	\N	118	\N	158	\N	1	9	<databases><nosql><neo4j>	Is this Neo4j comparison to RDBMS execution time correct?	572
46	\N	\N	<p>Overfitting, in a nutshell, means take into account <strong>too much</strong> information from your data and/or prior knowledge, and use it in a model. To make it more straightforward, consider the following example: you're hired by some scientists to provide them with a model to predict the growth of some kind of plants. The scientists have given you information collected from their work \nwith such plants throughout a whole year, and they shall continuously give you information on the future development of their plantation.</p>\n\n<p>So, you run through the data received, and build up a model out of it. Now suppose that, in your model, you considered just as many characteristics as possible to always find out the exact behavior of the plants you saw in the initial dataset. Now, as the production continues, you'll always take into account those characteristics, and will produce very <em>fine-grained</em> results. However, if the plantation eventually suffer from some seasonal change, the results you will receive may fit your model in such a way that your predictions will begin to fail (either saying that the growth will slow down, while it shall actually speed up, or the opposite).</p>\n\n<p>Apart from being unable to detect such small variations, and to usually classify your entries incorrectly, the <em>fine-grain</em> on the model, i.e., the great amount of variables, may cause the processing to be too costly. Now, imagine that your data is already complex. Overfitting your model to the data not only will make the classification/evaluation very complex, but will most probably make you error the prediction over the slightest variation you may have on the input.</p>\n\n<p><strong>Edit</strong>: <a href="https://www.youtube.com/watch?v=DQWI1kvmwRg">This</a> might as well be of some use, perhaps adding dynamicity to the above explanation :D</p>\n	\N	0	\N	2014-05-14T18:37:52.333	\N	64	2014-05-15T23:22:39.427	2014-05-15T23:22:39.427	\N	84	\N	84	61	2	17	\N	\N	\N
47	\N	\N		\N	0	\N	2014-05-14T18:45:23.917	\N	65	2014-05-14T18:45:23.917	2014-05-14T18:45:23.917	\N	-1	\N	-1	\N	5	0	\N	\N	\N
48	\N	\N	Big data is the term for a collection of data sets so large and complex that it becomes difficult to process using on-hand database management tools or traditional data processing applications. The challenges include capture, curation, storage, search, sharing, transfer, analysis and visualization.	\N	0	\N	2014-05-14T18:45:23.917	\N	66	2014-05-16T13:45:57.450	2014-05-16T13:45:57.450	\N	118	\N	118	\N	4	0	\N	\N	\N
49	\N	\N		\N	0	\N	2014-05-14T18:48:42.263	\N	67	2014-05-14T18:48:42.263	2014-05-14T18:48:42.263	\N	-1	\N	-1	\N	5	0	\N	\N	\N
50	\N	\N		\N	0	\N	2014-05-14T18:48:42.263	\N	68	2014-05-14T18:48:42.263	2014-05-14T18:48:42.263	\N	-1	\N	-1	\N	4	0	\N	\N	\N
51	\N	1	<p>First, think it's worth me stating what I mean by replication &amp; reproducibility:</p>\n\n<ul>\n<li>Replication of analysis A results in an exact copy of all inputs and processes that are supply and result in incidental outputs in analysis B.</li>\n<li>Reproducibility of analysis A results in inputs, processes, and outputs that are semantically incidental to analysis A, without access to the exact inputs and processes.</li>\n</ul>\n\n<p>Putting aside how easy it might be to replicate a given build, especially an ad-hoc one, to me replication always possible if it's planned for and worth doing. That said, it is unclear to me is how to execute a data science workflow that allows for reproducibility.</p>\n\n<p>The closet comparison I'm able to think of is <a href="http://en.wikipedia.org/wiki/Documentation_generator" rel="nofollow noreferrer">documentation generators</a> that generates software documentation intended for programmers - though the main difference I see is that in theory, if two sets of analysis ran the "reproducibility documentation generators" the documentation should match.</p>\n\n<p>Another issue, is that while I get the concept of reproducibility documentation, I am having a hard time imagining what it would look like in usable form without just being a guide to replicating the analysis.</p>\n\n<p>Lastly, whole intent of this is to understand if it's possible to "bake-in" reproducibility documentation as you build out a stack, not after the stack is built.</p>\n\n<p>So, Is it possible to automate generating reproducibility documentation, and if so how, and what would it look like?</p>\n\n<hr>\n\n<p><strong><em>UPDATE:</strong> Please note that this is the second draft of this question and that <a href="https://datascience.stackexchange.com/users/178/christopher-louden">Christopher Louden</a> was kind enough to let me edit the question after I realized it was likely the first draft was unclear. Thanks!</em></p>\n	\N	2	\N	2014-05-14T20:03:15.233	\N	69	2014-05-15T02:02:08.010	2017-04-13T12:50:41.230	\N	-1	\N	158	\N	1	2	<processing>	Is it possible to automate generating reproducibility documentation?	80
52	\N	\N	<p>To be reproducible without being just a replication, you would need to redo the experiment with new data, following the same technique as before.  The work flow is not as important as the techniques used.  Sample data in the same way, use the same type of models.  It doesn't matter if you switch from one language to another, so long as the models and the data manipulations are the same.</p>\n\n<p>This type of replication will show that the results you got in the first experiment are less likely to be a fluke than they were earlier.</p>\n	\N	4	\N	2014-05-14T22:03:50.597	\N	70	2014-05-14T22:03:50.597	\N	\N	\N	\N	178	69	2	2	\N	\N	\N
53	84	3	<p>What are the data conditions that we should watch out for, where p-values may not be the best way of deciding statistical significance?  Are there specific problem types that fall into this category?</p>\n	\N	3	\N	2014-05-14T22:12:37.203	3	71	2014-05-15T08:25:47.933	\N	\N	\N	\N	179	\N	1	13	<bigdata><statistics>	When are p-values deceptive?	364
54	\N	\N	<p>One algorithm that can be used for this is the <a href="http://en.wikipedia.org/wiki/K-means_clustering" rel="noreferrer">k-means clustering algorithm</a>.</p>\n\n<p>Basically:</p>\n\n<ol>\n<li>Randomly choose k datapoints from your set, m_1, ..., m_k.</li>\n<li><p>"Until convergence":</p>\n\n<ol>\n<li>Assign your data points to k clusters, where cluster i is the set of points for which m_i is the closest of your current means</li>\n<li>Replace each m_i by the mean of all points assigned to cluster i.</li>\n</ol></li>\n</ol>\n\n<p>It is good practice to repeat this algorithm several times, then choose the outcome that minimizes distances between the points of each cluster i and the center m_i.</p>\n\n<p>Of course, you have to know k to start here; you can use cross-validation to choose this parameter, though.</p>\n	\N	0	\N	2014-05-14T22:40:40.363	\N	72	2014-05-14T22:40:40.363	\N	\N	\N	\N	22	31	2	5	\N	\N	\N
55	\N	\N	<p>You shouldn't consider the p-value out of context.</p>\n\n<p>One rather basic point (as illustrated by <a href="http://xkcd.com/882/" rel="nofollow">xkcd</a>) is that you need to consider how many tests you're actually doing.  Obviously, you shouldn't be shocked to see p &lt; 0.05 for one out of 20 tests, even if the null hypothesis is true every time.  </p>\n\n<p>A more subtle example of this occurs in high-energy physics, and is known as the <a href="https://en.wikipedia.org/wiki/Look-elsewhere_effect" rel="nofollow">look-elsewhere effect</a>.  The larger the parameter space you search for a signal that might represent a new particle, the more likely you are to see an apparent signal that's really just due to random fluctuations. </p>\n	\N	0	\N	2014-05-14T22:43:23.587	\N	73	2014-05-14T22:43:23.587	\N	\N	\N	\N	14	71	2	4	\N	\N	\N
56	\N	\N	<p>One thing you should be aware of is the sample size you are using. Very large samples, such as economists using census data, will lead to deflated p-values. This paper <a href="http://galitshmueli.com/system/files/Print%20Version.pdf" rel="nofollow" title="Too Big to Fail: Large Samples and the p-Value Problem">"Too Big to Fail: Large Samples and the p-Value Problem"</a> covers some of the issues. </p>\n	\N	0	\N	2014-05-14T22:58:11.583	\N	74	2014-05-14T22:58:11.583	\N	\N	\N	\N	64	71	2	2	\N	\N	\N
57	78	2	<p>If small p-values are plentiful in big data, what is a comparable replacement for p-values in data with million of samples?</p>\n	\N	0	\N	2014-05-15T00:26:11.387	1	75	2014-05-15T20:32:26.923	\N	\N	\N	\N	158	\N	1	4	<statistics>	Is there a replacement for small p-values in big data?	137
60	\N	\N	<p>There is no replacement in the strict sense of the word.  Instead you should look at other measures.</p>\n\n<p>The other measures you look at depend on what you type of problem you are solving.  In general, if you have a small p-value, also consider the magnitude of the effect size.  It may be highly statistically significant but in practice meaningless.  It is also helpful to report the confidence interval of the effect size.</p>\n\n<p>I would consider <a href="http://galitshmueli.com/system/files/Print%20Version.pdf" rel="nofollow noreferrer">this paper</a> as mentoned in DanC's answer to <a href="https://datascience.stackexchange.com/questions/71/when-are-p-values-deceptive">this question</a>.</p>\n	\N	0	\N	2014-05-15T01:46:28.467	\N	78	2014-05-15T01:46:28.467	2017-04-13T12:50:41.230	\N	-1	\N	178	75	2	5	\N	\N	\N
61	\N	\N	<p>Conceptually speaking, <em>data-mining</em> can be thought of as one item (or set of skills and applications) in the toolkit of the data scientist.</p>\n\n<p>More specifically, data-mining is an activity that seeks patterns in large, complex data sets. It usually emphasizes algorithmic techniques, but may also involve any set of related skills, applications, or methodologies with that goal.</p>\n\n<p>In US-English colloquial speech, data-mining and data-collection are often used interchangeably.</p>\n\n<p>However, a main difference between these two related activities is <em>intentionality</em>. </p>\n\n<p><em>Definition inspired mostly by the contributions of <a href="https://datascience.stackexchange.com/users/36/statsrus">@statsRus</a> to Data Science.SE</em></p>\n	\N	0	\N	2014-05-15T03:19:40.360	\N	79	2017-08-27T17:25:18.230	2017-08-27T17:25:18.230	\N	3117	\N	53	\N	5	0	\N	\N	\N
62	\N	\N	An activity that seeks patterns in large, complex data sets. It usually emphasizes algorithmic techniques, but may also involve any set of related skills, applications, or methodologies with that goal.	\N	0	\N	2014-05-15T03:19:40.360	\N	80	2014-05-16T13:46:05.850	2014-05-16T13:46:05.850	\N	53	\N	53	\N	4	0	\N	\N	\N
63	82	3	<p>What is(are) the difference(s) between parallel and distributed computing? When it comes to scalability and efficiency, it is very common to see solutions dealing with computations in clusters of machines, and sometimes it is referred to as a parallel processing, or as distributed processing.</p>\n\n<p>In a certain way, the computation seems to be always parallel, since there are things running concurrently. But is the distributed computation simply related to the use of more than one machine, or are there any further specificities that distinguishes these two kinds of processing? Wouldn't it be redundant to say, for example, that a computation is <em>parallel AND distributed</em>?</p>\n	\N	0	\N	2014-05-15T04:59:54.317	\N	81	2014-05-18T17:38:01.383	2014-05-15T09:31:51.370	\N	118	\N	84	\N	1	15	<definitions><parallel><distributed>	Parallel and distributed computing	591
64	\N	\N	<p>Simply set, 'parallel' means running concurrently on distinct resources (CPUs), while 'distributed' means running across distinct computers, involving issues related to networks.</p>\n\n<p>Parallel computing using for instance <a href="http://en.wikipedia.org/wiki/OpenMP">OpenMP</a> is not distributed, while parallel computing with <a href="http://en.wikipedia.org/wiki/Message_Passing_Interface">Message Passing</a> is often distributed.</p>\n\n<p>Being in a 'distributed but not parallel' setting would mean under-using resources so it is seldom encountered but it is conceptually possible.</p>\n	\N	0	\N	2014-05-15T05:19:34.757	\N	82	2014-05-15T05:25:39.970	2014-05-15T05:25:39.970	\N	172	\N	172	81	2	16	\N	\N	\N
65	\N	\N	<p>I posted a pretty detailed answer on stackoverflow about when it is appropriate to use relational vs document (or NoSQL) database, here:  </p>\n\n<p><strong><a href="https://stackoverflow.com/questions/13528216/motivations-for-using-relational-database-orm-or-document-database-odm/13599767#13599767">Motivations for using relational database / ORM or document database / ODM</a></strong></p>\n\n<p>Summary:</p>\n\n<ul>\n<li><p>for small stuff, go with whatever tools you are familiar with</p></li>\n<li><p>a few gigabytes is definitely small stuff: it doesn't get big until it is too big to fit in a single <a href="http://www.mysql.com/products/cluster/scalability.html" rel="nofollow noreferrer">MySQL Cluster</a> with a reasonable number of nodes (16-32), which means maybe 8-16TB data and a few million transactions per second (or a more conventional hard-drive-based database with up to 100's of TB data and a few thousand transactions per second).</p></li>\n<li><p>if you're stuck with another database (not MySQL Cluster), get more mileage out of it by throwing in FusionIO hardware.</p></li>\n<li><p>once you have data larger than a few TB <em>and</em> faster than thousands of transactions per second, it is a good time to look at moving to logical sharding in the application code first and then to NoSQL.</p></li>\n<li><p><a href="http://cassandra.apache.org/" rel="nofollow noreferrer">Cassandra</a> :)</p></li>\n</ul>\n	\N	0	\N	2014-05-15T07:47:44.710	\N	83	2014-05-15T07:59:05.497	2017-05-23T12:38:53.587	\N	-1	\N	26	20	2	8	\N	\N	\N
66	\N	\N	<p>You are asking about <a href="http://en.wikipedia.org/wiki/Data_dredging">Data Dredging</a>, which is what happens when testing a very large number of hypotheses against a data set, or testing hypotheses against a data set that were suggested by the same data.  </p>\n\n<p>In particular, check out <a href="http://en.wikipedia.org/wiki/Multiple_comparisons">Multiple hypothesis hazard</a>, and <a href="http://en.wikipedia.org/wiki/Testing_hypotheses_suggested_by_the_data">Testing hypotheses suggested by the data</a>.</p>\n\n<p>The solution is to use some kind of correction for <a href="http://en.wikipedia.org/wiki/False_discovery_rate">False discovery rate</a> or <a href="http://en.wikipedia.org/wiki/Familywise_error_rate">Familywise error rate</a>, such as <a href="http://en.wikipedia.org/wiki/Scheff%C3%A9%27s_method">Scheffé's method</a> or the (very old-school) <a href="http://en.wikipedia.org/wiki/Bonferroni_correction">Bonferroni correction</a>.</p>\n\n<p>In a somewhat less rigorous way, it may help to filter your discoveries by the confidence interval for the odds ratio (OR) for each statistical result.  If the 99% confidence interval for the odds ratio is 10-12, then the OR is &lt;= 1 with some <em>extremely</em> small probability, especially if the sample size is also large.  If you find something like this, it is probably a strong effect even if it came out of a test of millions of hypotheses.  </p>\n	\N	1	\N	2014-05-15T08:19:40.577	\N	84	2014-05-15T08:25:47.933	2014-05-15T08:25:47.933	\N	26	\N	26	71	2	8	\N	\N	\N
80	111	1	<p>What is the best noSQL backend to use for a mobile game? Users can make a lot of servers requests, it needs also to retrieve users' historical records (like app purchasing) and analytics of usage behavior.</p>\n	\N	3	\N	2014-05-16T05:09:33.557	\N	102	2016-12-09T21:55:46.000	2014-05-18T19:41:19.157	\N	229	\N	199	\N	1	6	<nosql><performance>	What is the Best NoSQL backend for a mobile game	449
82	\N	\N	<h2>Use the <a href="/questions/tagged/definitions" class="post-tag" title="show questions tagged &#39;definitions&#39;" rel="tag">definitions</a> tag when:</h2>\n\n<p>You think we should create an official definition.</p>\n\n<p>An existing Tag Wiki needs a more precise definition to avoid confusion and we need to create consensus before an edit.</p>\n\n<p>(rough draft - needs filling out)</p>\n	\N	0	\N	2014-05-16T15:35:51.420	\N	104	2014-05-20T13:50:52.447	2014-05-20T13:50:52.447	\N	53	\N	53	\N	5	0	\N	\N	\N
83	\N	\N	a discussion (meta) tag used when there exists *disagreement* or *confusion* about the everyday meaning of a term or phrase.	\N	0	\N	2014-05-16T15:35:51.420	\N	105	2014-05-20T13:53:05.697	2014-05-20T13:53:05.697	\N	53	\N	53	\N	4	0	\N	\N	\N
67	\N	\N	<p>See also <a href="https://datascience.stackexchange.com/questions/71/when-are-p-values-deceptive/84#84">When are p-values deceptive?</a></p>\n\n<p>When there are a lot of variables that can be tested for pair-wise correlation (for example), the replacement is to use any of the corrections for <a href="http://en.wikipedia.org/wiki/False_discovery_rate" rel="nofollow noreferrer">False discovery rate</a> (to limit probability that any given discovery is false) or <a href="http://en.wikipedia.org/wiki/Familywise_error_rate" rel="nofollow noreferrer">Familywise error rate</a> (to limit probability of one or more false discoveries).  For example, you might use the Holm–Bonferroni method.</p>\n\n<p>In the case of a large sample rather than a lot of variables, something else is needed.  As Christopher said, magnitude of effect a way to treat this.  Combining these two ideas, you might use a confidence interval around your magnitude of effect, and apply a false discovery rate correction to the p-value of the confidence interval.  The effects for which even the lowest bound of the corrected confidence interval is high are likely to be strong effects, regardless of huge data set size.  I am not aware of any published paper that combines confidence intervals with false discovery rate correction in this way, but it seems like a straightforward and intuitively understandable approach.</p>\n\n<p>To make this even better, use a non-parametric way to estimate confidence intervals.  Assuming a distribution is likely to give very optimistic estimates here, and even fitting a distribution to the data is likely to be inaccurate.  Since the information about the shape of the distribution past the edges of the confidence interval comes from a relatively small subsample of the data, this is where it really pays to be careful.  You can use bootstrapping to get a non-parametric confidence interval.</p>\n	\N	1	\N	2014-05-15T08:44:47.327	\N	85	2014-05-15T20:32:26.923	2017-04-13T12:50:41.230	\N	-1	\N	26	75	2	3	\N	\N	\N
68	101	2	<p>Given website access data in the form <code>session_id, ip, user_agent</code>, and optionally timestamp, following the conditions below, how would you best cluster the sessions into unique visitors?</p>\n\n<p><code>session_id</code>: is an id given to every new visitor. It does not expire, however if the user doesn't accept cookies/clears cookies/changes browser/changes device, he will not be recognised anymore</p>\n\n<p><code>IP</code> can be shared between different users (Imagine a free wi-fi cafe, or your ISP reassigning IPs), and they will often have at least 2, home and work.</p>\n\n<p><code>User_agent</code> is the browser+OS version, allowing to distinguish between devices. For example a user is likely to use both phone and laptop, but is unlikely to use windows+apple laptops. It is unlikely that the same session id has multiple useragents.</p>\n\n<p>Data might look as the fiddle here:\n<a href="http://sqlfiddle.com/#!2/c4de40/1">http://sqlfiddle.com/#!2/c4de40/1</a></p>\n\n<p>Of course, we are talking about assumptions, but it's about getting as close to reality as possible. For example, if we encounter the same ip and useragent in a limited time frame with a different session_id, it would be a fair assumption that it's the same user, with some edge case exceptions.</p>\n\n<p>Edit: Language in which the problem is solved is irellevant, it's mostly about logic and not implementation. Pseudocode is fine.</p>\n\n<p>Edit: due to the slow nature of the fiddle, you can alternatively read/run the mysql:</p>\n\n<pre><code>select session_id, floor(rand()*256*256*256*256) as ip_num , floor(rand()*1000) as user_agent_id\nfrom \n    (select 1+a.nr+10*b.nr as session_id, ceil(rand()*3) as nr\n    from\n        (select 1 as nr union all select 2 union all select 3   union all select 4 union all select 5\n        union all select 6 union all select 7 union all select 8 union all select 9 union all select 0)a\n    join\n        (select 1 as nr union all select 2 union all select 3   union all select 4 union all select 5\n        union all select 6 union all select 7 union all select 8 union all select 9 union all select 0)b\n        order by 1\n    )d\ninner join\n    (select 1 as nr union all select 2 union all select 3   union all select 4 union all select 5\n    union all select 6 union all select 7 union all select 8 union all select 9 )e\n    on d.nr&gt;=e.nr\n</code></pre>\n	\N	0	\N	2014-05-15T09:04:09.710	2	86	2014-05-15T21:41:22.703	2014-05-15T10:06:06.393	\N	116	\N	116	\N	1	12	<clustering>	Clustering unique visitors by useragent, ip, session_id	1451
69	\N	\N	<p>Looking at this document called <a href="https://www.facebook.com/notes/facebook-data-team/anatomy-of-facebook/10150388519243859">Anatomy of Facebook</a> I note that the median is 100. Looking at the cumulative function plot I can bet that the average is higher, near 200. So 50 seems to not be the best number here. However I think that this is not the main issue here. </p>\n\n<p>The main issue is the lack of information on how the database was used.</p>\n\n<p>It seems reasonable that a data storage designed specially for graph structures to be more efficient than traditional RDBMs. However, even if the RDBMs are not in the latest trends as a data storage of choice, these systems evolved continuously in a race with the data set dimensions. There are various types of possible designs, various ways of indexing data, improvements related with concurrency and so on. </p>\n\n<p>To conclude I think that regarding reproducibility, the study lack a proper description of how the database schema was designed. I do not expect that a database to dominate on such king of interrogations, however I would expect that with a well-tuned design the differences to not be such massive.</p>\n	\N	0	\N	2014-05-15T09:30:36.460	\N	87	2014-05-15T09:30:36.460	\N	\N	\N	\N	108	77	2	7	\N	\N	\N
70	91	3	<p>For example, when searching something in Google, results return nigh-instantly.</p>\n\n<p>I understand that Google sorts and indexes pages with algorithms etc., but I imagine it infeasible for the results of every single possible query to be indexed (and results are personalized, which renders this even more infeasible)?</p>\n\n<p>Moreover, wouldn't the hardware latency in Google's hardware be huge? Even if the data in Google were all stored in TB/s SSDs, I imagine the hardware latency to be huge, given the sheer amount of data to process.</p>\n\n<p>Does MapReduce help solve this problem?</p>\n\n<p>EDIT: Okay, so I understand that popular searches can be cached in memory. But what about unpopular searches? Even for the most obscure search I have conducted, I don't think the search has ever been reported to be larger than 5 seconds. How is this possible?</p>\n	\N	0	\N	2014-05-15T11:22:27.293	\N	89	2014-08-30T18:40:02.403	2014-05-16T02:46:56.510	\N	189	\N	189	\N	1	10	<bigdata><google><search>	How does a query into a huge database return with negligible latency?	334
71	\N	\N	<p>The terms "parallel computing" and "distributed computing" certainly have a large overlap, but can be differentiated further. Actually, you already did this in your question, by later asking about "parallel processing" and "distributed processing". </p>\n\n<p>One could consider "distributed computing" as the more general term that involves "distributed processing" as well as, for example, "distributed storage". The common term, "distributed", usually refers to some sort of <a href="http://en.wikipedia.org/wiki/Message_Passing_Interface" rel="noreferrer">Message Passing</a> over a network, between machines that are physically separated.</p>\n\n<p>The term "parallel computing" is also in the process of being further defined, e.g. by explicitly differentiating between the terms "parallel" and "concurrent", where - roughly - the first one refers <a href="http://en.wikipedia.org/wiki/Data_parallelism" rel="noreferrer">data parallelism</a> and the latter to <a href="http://en.wikipedia.org/wiki/Task_parallelism" rel="noreferrer">task parallelism</a>, although there are hardly really strict and binding defintions. </p>\n\n<p>So one could say that </p>\n\n<ul>\n<li>"distributed processing" usually (although not necessarily) means that it also is "parallel processing"</li>\n<li>"distributed computing" is more general, and also covers aspects that are not related to parallelism</li>\n<li>and obviously, "parallel computing"/"parallel processing" does not imply that it is "distributed"</li>\n</ul>\n	\N	0	\N	2014-05-15T11:46:38.170	\N	90	2014-05-15T11:46:38.170	\N	\N	\N	\N	156	81	2	4	\N	\N	\N
72	\N	\N	<p>Well, I'm not sure if it is MapReduce that solves the problem, but it surely wouldn't be MapReduce alone to solve all these questions you raised. But here are important things to take into account, and that make it <em>feasible</em> to have such low latency on queries from all these TBs of data in different machines:</p>\n\n<ol>\n<li>distributed computing: by being distributed does not mean that the indexes are simply distributed in different machines, they are actually replicated along different clusters, which allows for lots of users performing different queries with low retrieval time (yes, huge companies can afford for that much of machines);</li>\n<li>caching: caches tremendously reduce execution time, be it for the crawling step, for the retrieval of pages, or for the ranking and exihibition of results;</li>\n<li>lots of tweaking: all the above and very efficient algorithms/solutions can only be effective if the implementation is also efficient. There are tons of (hard coded) optimizations, such as locality of reference, compression, caching; all of them usually appliable to different parts of the processing.</li>\n</ol>\n\n<p>Considering that, lets try to address your questions:</p>\n\n<blockquote>\n  <p>but I imagine it infeasible for the results of every single possible query to be indexed</p>\n</blockquote>\n\n<p>Yes, it would be, and actually is infeasible to have results for <em>every single possible query</em>. There is an infinite number of terms in the world (even if you assume that only terms properly spelled will be entered), and there is an exponential number of queries from these <code>n -&gt; inf</code> terms (<code>2^n</code>). So what is done? Caching. But if there are so many queries/results, which ones to cache? Caching policies. The most frequent/popular/relevant-for-the-user queries are the ones cached.</p>\n\n<blockquote>\n  <p>wouldn't the hardware latency in Google's hardware be huge? Even if the data in Google were all stored in TB/s SSDs</p>\n</blockquote>\n\n<p>Nowdays, with such highly developed processors, people tend to think that every possible task that must finish within a second (or less), and that deals with so much data, must be processed by extremely powerful processors with multiple cores and lots of memory. However, the one thing <em>ruling</em> market is money, and the investors are not interested in wasting it. So what is done?</p>\n\n<p>The preference is actually for having lots of machines, each using simple/accessible (in terms of cost) processors, which lowers the price of building up the multitude of clusters there are. And yes, it does work. The main bottleneck always boils down to disk, if you consider simple measurements of <a href="https://i.stack.imgur.com/Uf6al.gif" rel="noreferrer">performance</a>. But once there are so many machines, one can afford to load things up to main memory, instead of working on hard disks.</p>\n\n<p>Memory cards are <em>expensive</em> for us, mere human beings, but they are very cheap for enterprises that buy lots of such cards at once. Since it's not costly, having much memory as needed to load indexes and keep caches at hand is not a problem. And since there are so many machines, there is no need for super fast processors, as you can direct queries to different places, and have clusters of machines responsible for attending <em>specific geographical regions</em>, which allows for more <em>specialized</em> data caching, and even better response times.</p>\n\n<blockquote>\n  <p>Does MapReduce help solve this problem?</p>\n</blockquote>\n\n<p>Although I don't think that using or not MapReduce is restricted information inside Google, I'm not conversant about this point. However, Google's implementation of MapReduce (which is surely <em>not</em> Hadoop) must have lots of optimizations, many involving the aspects discussed above. So, the architecture of MapReduce probably helps guiding how the computations are physically distributed, but there are many other points to be considered to justify such speed in querying time.</p>\n\n<blockquote>\n  <p>Okay, so I understand that popular searches can be cached in memory. But what about unpopular searches?</p>\n</blockquote>\n\n<p>The graph below presents a curve of how the <em>kinds</em> of queries occur. You can see that there are three main kinds of searches, each of them holding approximately 1/3 of the volume of queries (area below curve). The plot shows power law, and reinforces the fact that smaller queries are the most popular. The second third of queries are still feasible to process, since they hold few words. But the set of so-called <em>obscure queries</em>, which usually consist of non-experienced users' queries, are not a negligible part of the queries.</p>\n\n<p><img src="https://i.stack.imgur.com/CpcNf.jpg" alt="Heavy-tailed distribution"></p>\n\n<p>And there lies space for novel solutions. Since it's not just one or two queries (but one third of them), they must have <em>relevant</em> results. If you type in something <em>much too obscure</em> in a Google search, it shan't take longer to return a list of results, but will most probably show you something it <em>inferred</em> you'd like to say. Or it may simply state that there was no document with such terms -- or even cut down your search to 32 words (which just happened to me in a random test here).</p>\n\n<p>There are dozens of appliable heuristics, which may be either to ignore some words, or to try to break the query into smaller ones, and gather the most <em>popular</em> results. And all these solutions can be tailored and tweaked to respect <em>feasible waiting times</em> of, say, less then a second? :D</p>\n	\N	2	\N	2014-05-15T11:56:43.607	\N	91	2014-05-16T04:33:52.310	2014-05-16T04:33:52.310	\N	84	\N	84	89	2	12	\N	\N	\N
81	\N	4	<p>Assume that we have a set of elements <em>E</em> and a similarity (<strong>not distance</strong>) function <em>sim(ei, ej)</em> between two elements <em>ei,ej ∈ E</em>. </p>\n\n<p>How could we (efficiently) cluster the elements of <em>E</em>, using <em>sim</em>?</p>\n\n<p><em>k</em>-means, for example, requires a given <em>k</em>, Canopy Clustering requires two threshold values. What if we don't want such predefined parameters?</p>\n\n<p>Note, that <em>sim</em> is not neccessarily a metric (i.e. the triangle inequality may, or may not hold). Moreover, it doesn't matter if the clusters are disjoint (partitions of <em>E</em>).</p>\n	\N	7	\N	2014-05-16T14:26:12.270	2	103	2014-06-25T15:53:30.723	\N	\N	\N	\N	113	\N	1	16	<clustering><algorithms><similarity>	Clustering based on similarity scores	3861
73	\N	\N	<p>MapReduce has nothing to do with real-time anything. It is a batch-oriented processing framework suitable for some offline tasks, like ETL and index building. Google has moved off of MapReduce for most jobs now, and even the Hadoop ecosystem is doing the same.</p>\n\n<p>The answer to low latency is generally to keep precomputed indices in memory. Anything that touches disk is hard to make fast and scale. This is how newer-generation Hadoop-based SQL engines like <a href="http://www.cloudera.com/content/cloudera/en/products-and-services/cdh/impala.html">Impala</a> get so much speed compared to MapReduce-based infrastructure like <a href="http://hive.apache.org/">Hive</a>, for example.</p>\n\n<p>Search infrastructure can't cache the results of every single query. But it sure can cache intermediate results, or, more complete results for top queries. With a little caching you can serve results for a significant minority of all queries.</p>\n\n<p>Search is also split across servers. So one machine can delegate to 100 to each get a part of the result and then combine them.</p>\n\n<p>You can also get away with some degree of approximation. Google does not literally form a thousand pages of search results; it just has to get the first page about right.</p>\n\n<p>Keep in mind that Google has <em>millions</em> of computers around the globe. Your queries are going to a data center geographically near to you and that is only serving your geography. This cuts out most of the latency, which is network and not processing time in the data center.</p>\n	\N	3	\N	2014-05-15T13:18:38.693	\N	92	2014-05-15T13:18:38.693	\N	\N	\N	\N	21	89	2	9	\N	\N	\N
74	\N	\N	<p>There's not much you can do with just this data, but what little you can do does not rely on machine learning. </p>\n\n<p>Yes, sessions from the same IP but different User-Agents are almost certainly distinct users. Sessions with the same IP and User-Agent are usually the same user, except in the case of proxies / wi-fi access points. Those you might identify by looking at the distribution of session count per IP to identify likely 'aggregate' IPs. Sessions from the same IP / User-Agent that overlap in time are almost surely distinct.</p>\n\n<p>To further distinguish users you would need more info. For example, the sites or IP addresses that the user is connecting to would be a very strong basis for differentiating sessions. Then you could get into more sophisticated learning to figure out when sessions are the same or different users.</p>\n	\N	3	\N	2014-05-15T13:30:04.270	\N	93	2014-05-15T13:30:04.270	\N	\N	\N	\N	21	86	2	6	\N	\N	\N
75	97	3	<p>While building a rank, say for a search engine, or a recommendation system, is it valid to rely on click frequency to determine the relevance of an entry?</p>\n	\N	0	\N	2014-05-15T14:41:24.020	1	94	2015-11-23T15:36:28.760	\N	\N	\N	\N	84	\N	1	17	<recommender-system><information-retrieval>	Does click frequency account for relevance?	223
76	\N	\N	<p>Is it valid to <em>use</em> click frequency, then <strong>yes</strong>. Is it valid to use <strong>only</strong> the click frequency, then probably <strong>no</strong>.</p>\n\n<p>Search relevance is much more complicated than just one metric. <a href="http://www.amazon.ca/s/ref=nb_sb_noss?url=search-alias=aps&amp;field-keywords=search%20ranking" rel="noreferrer">There are entire books on the subject</a>. Extending this answer beyond a simple yes/no would likely make the answer far too broad (and opinionated)</p>\n	\N	0	\N	2014-05-15T15:06:24.600	\N	95	2014-05-15T15:06:24.600	\N	\N	\N	\N	9	94	2	5	\N	\N	\N
77	\N	\N	<p>For my part I can say that I use click frequency on i.e. eCommerce products. When you combine it with the days of the year it can even bring you great suggestions.</p>\n\n<p>i.e.: We have historical data from 1 year over 2 products (Snowboots[], Sandalettes[])</p>\n\n<pre><code>Snowboots[1024,1253,652,123,50,12,8,4,50,148,345,896]\nSandalettes[23,50,73,100,534,701,1053,1503,1125,453,213,25]\n</code></pre>\n\n<p>where [0] = January</p>\n\n<p>As you can see, snowboots are much more searched in January than sandalettes, so you should suggest snowboots to someone searching shoes on your site or /we on january.</p>\n\n<p>You can also see if something is "fresh" at this time, like when people often click a unknown product it could be an insight for a new comming trend or something.</p>\n\n<p>That are just some examples where you could use click frequency as an insight. I think there are <strong>no rules</strong> for what you can use or not in recommendations, <strong>as long as it makes sense</strong>.</p>\n	\N	0	\N	2014-05-15T15:10:30.243	\N	96	2015-11-23T15:36:28.760	2015-11-23T15:36:28.760	\N	115	\N	115	94	2	7	\N	\N	\N
78	\N	\N	<p><a href="http://research.microsoft.com/pubs/169639/cikm-clickpatterns.pdf" rel="noreferrer">Depends on the user's intent</a>, for starters. </p>\n\n<p><a href="http://www.seoresearcher.com/distribution-of-clicks-on-googles-serps-and-eye-tracking-analysis.htm" rel="noreferrer">Users normally only view the first set of links</a>, which means that unless the link is viewable, it's not getting clicks; meaning you'd have to be positive those are the best links, otherwise the clicks are most likely going to reflect placement, not relevance. For example, here's a <a href="http://www.seoresearcher.com/distribution-of-clicks-on-googles-serps-and-eye-tracking-analysis.htm" rel="noreferrer">click and attention distribution heat-map</a> for  Google search results:</p>\n\n<p><img src="https://i.stack.imgur.com/8kO5S.jpg" alt="Google SEPR Click and Attention distribution ‘heat-map’"></p>\n\n<p>Further, using click frequency to account for relevance is not a direct measure of the resource's relevance. Also, using clicks is problematic, since issues like click-inflation, click-fraud, etc. will pop-up and are hard to counter. </p>\n\n<p>That said, if you're interested in using user interaction to model relevance, I would suggest you attempt to measure post-click engagement, not how users respond to search results; see "<a href="http://www.youtube.com/watch?v=BsCeNCVb-d8" rel="noreferrer">YouTube's head of engineering speaking about clicks vs engagement</a>" for more information, though note that the <a href="http://www.orbitmedia.com/blog/ideal-blog-post-length/" rel="noreferrer">size itself of the content is a factor</a> too.</p>\n\n<p>Might be worth noting that historically Google was known for <a href="http://en.wikipedia.org/wiki/PageRank" rel="noreferrer">PageRank algorithm</a> though it's possible your intent is only to review click-streams, so I won't delve <a href="https://www.google.com/search?q=google%20ranking%20factors" rel="noreferrer">Google ranking factors</a>; if you are interested in the Google's approach, you might find a review of <a href="http://static.googleusercontent.com/media/www.google.com/en/us/insidesearch/howsearchworks/assets/searchqualityevaluatorguidelines.pdf" rel="noreferrer">Google's Search Quality Rating Guidelines</a>.</p>\n	\N	0	\N	2014-05-15T17:14:36.817	\N	97	2014-05-15T23:08:04.300	2014-05-15T23:08:04.300	\N	158	\N	158	94	2	14	\N	\N	\N
79	\N	\N	<p>One possibility here (and this is really an extension of what Sean Owen posted) is to define a "stable user."</p>\n\n<p>For the given info you have you can imagine making a user_id that is a hash of ip and some user agent info (pseudo code):</p>\n\n<pre><code>uid = MD5Hash(ip + UA.device + UA.model)\n</code></pre>\n\n<p>Then you flag these ids with "stable" or "unstable" based on usage heuristics you observe for your users.  This can be a threshold of # of visits in a given time window, length of time their cookies persist, some end action on your site (I realize this wasn't stated in your original log), etc...</p>\n\n<p>The idea here is to separate the users that don't drop cookies from those that do.</p>\n\n<p>From here you can attribute session_ids to stable uids from your logs.  You will then have "left over" session_ids for unstable users that you are relatively unsure about.  You may be over or under counting sessions, attributing behavior to multiple people when there is only one, etc...  But this is at least limited to the users you are now "less certain" about.</p>\n\n<p>You then perform analytics on your stable group and project that to the unstable group.  Take a user count for example, you know the total # of sessions, but you are unsure of how many users generated those sessions.  You can find the # sessions / unique stable user and use this to project the "estimated" number of unique users in the unstable group since you know the number of sessions attributed to that group.</p>\n\n<pre><code>projected_num_unstable_users = num_sess_unstable / num_sess_per_stable_uid\n</code></pre>\n\n<p>This doesn't help with per user level investigation on unstable users but you can at least get some mileage out of a cohort of stable users that persist for some time.  You can, by various methods, project behavior and counts into the unstable group.  The above is a simple example of something you might want to know.  The general idea is again to define a set of users you are confident persist, measure what you want to measure, and use certain ground truths (num searches, visits, clicks, etc...) to project into the unknown user space and estimate counts for them.</p>\n\n<p>This is a longstanding problem in unique user counting, logging, etc... for services that don't require log in.</p>\n	\N	2	\N	2014-05-15T21:41:22.703	\N	101	2014-05-15T21:41:22.703	\N	\N	\N	\N	92	86	2	8	\N	\N	\N
84	\N	\N	<p>There are many overlaps between data mining and datascience. I would say that people with the role of datamining are concerned with data collection and the extraction of features from unfiltered, unorganised and mostly raw/wild datasets. Some very important data may be difficult to extract, not do to the implementation issues but because it may have foreign artifacts. </p>\n\n<p>Eg. if I needed someone to look at financial data from written tax returns in the 70s which were scanned and machine read to find out if people saved more on car insurance; a dataminer would be the person to get. </p>\n\n<p>If I needed someone to examine the influence Nike's Twitter profile in the tweets of Brazil and identify key positive features from the profile, I would look for a datascientist.</p>\n	\N	0	\N	2014-05-16T16:25:58.250	\N	106	2014-05-16T16:25:58.250	\N	\N	\N	\N	34	14	2	6	\N	\N	\N
85	\N	2	<p>Consider a stream containing <a href="http://en.m.wikipedia.org/wiki/Tuple" rel="nofollow">tuples</a> <code>(user, new_score)</code> representing users' scores in an online game. The stream could have 100-1,000 new elements per second. The game has 200K to 300K unique players. </p>\n\n<p>I would like to have some standing queries like: </p>\n\n<ol>\n<li>Which players posted more than x scores in a sliding window of one hour</li>\n<li>Which players gained x% score in a sliding window of one hour</li>\n</ol>\n\n<p>My question is which open source tools can I employ to jumpstart this project? I am considering <a href="http://esper.codehaus.org/" rel="nofollow">Esper</a> at the moment. </p>\n\n<p>Note: I have just completed reading "Mining Data Streams" (chapter 4 of <a href="http://infolab.stanford.edu/~ullman/mmds.html" rel="nofollow">Mining of Massive Datasets</a>) and I am quite new to mining data streams.</p>\n	\N	0	\N	2014-05-16T20:07:50.983	\N	107	2014-05-19T07:33:50.080	2014-05-19T07:33:50.080	\N	118	\N	200	\N	1	11	<tools><data-stream-mining>	Opensource tools for help in mining stream of leader board scores	103
86	\N	\N		\N	0	\N	2014-05-16T20:24:38.980	\N	108	2014-05-16T20:24:38.980	2014-05-16T20:24:38.980	\N	-1	\N	-1	\N	5	1	\N	\N	\N
87	\N	\N	An activity that seeks patterns in a continuous stream of data elements, usually involving summarizing the stream in some way.	\N	0	\N	2014-05-16T20:24:38.980	\N	109	2014-05-20T13:52:00.620	2014-05-20T13:52:00.620	\N	200	\N	200	\N	4	0	\N	\N	\N
88	\N	\N	<p>Some factors you might consider:</p>\n\n<p>Developer familiarity: go with whatever you or your developers are familiar with.  Mongo, Couch, Riak, DynamoDB etc all have their strengths but all should do ok here, so rather than going for an unfamiliar solution that might be slightly better go for familiar and save a bunch of development time.</p>\n\n<p>Ease of cloud deployment:  for example, if you are using Amazon AWS, then DynamoDB is likely an excellent choice.  Sure, you could use Mongo on AWS, but why bother?  Other cloud providers have their own preferred db, for example if you are using Google AppEngine, it makes sense to use BigTable or Cloud Datastore. </p>\n\n<p>Your use case seems both well suited to NoSQL and not very challenging since your data has a natural partition by user.  I think you'd be technically ok with anything, which is why I'm mainly covering other factors.</p>\n	\N	1	\N	2014-05-17T03:07:59.707	\N	111	2014-05-18T06:43:52.453	2014-05-18T06:43:52.453	\N	26	\N	26	102	2	8	\N	\N	\N
89	\N	\N	<p>This isn't a full solution, but you may want to look into <a href="http://www.orientechnologies.com/">OrientDB</a> as part of your stack. Orient is a Graph-Document database server written entirely in Java. </p>\n\n<p>In graph databases, relationships are considered first class citizens and therefore traversing those relationships can be done pretty quickly. Orient is also a document database which would allow you the kind of schema-free architecture it sounds like you would need. The real reason I suggest Orient, however, is because of its extensiblity. It supports streaming via sockets, and the entire database can be embedded into another application. Finally, it can be scaled efficiently and/or can work entirely through memory. So, with some Java expertise, you can actually run your preset queries against the database in memory.</p>\n\n<p>We are doing something similar. In creating an app/site for social science research collaboration, we found ourselves with immensely complex data models. We ended up writing several of the queries using the Gremlin Traversal Language (a subset of Groovy, which is, of course, Java at its heart), and then exposing those queries through the binary connection server of the OrientDB. So, the client opens a TCP socket, sends a short binary message, and the query is executing in Java directly against the in-memory database.</p>\n\n<p>OrientDB also supports writing function queries in Javascript, and you can use Node.js to interact directly with an Orient instance.</p>\n\n<p>For something of this size, I would want to use Orient in conjunction with Hadoop or something like that. You can also use Orient in conjunction with esper.</p>\n\n<p>Consider:\nAn introduction to orient: <a href="http://www.sitepoint.com/a-look-at-orientdb-the-graph-document-nosql/">http://www.sitepoint.com/a-look-at-orientdb-the-graph-document-nosql/</a></p>\n\n<p>Complex, real-time queries: <a href="http://www.gft-blog.com/business-trends/leveraging-real-time-scoring-through-bigdata-to-detect-insurance-fraud/">http://www.gft-blog.com/business-trends/leveraging-real-time-scoring-through-bigdata-to-detect-insurance-fraud/</a></p>\n\n<p>A discussion about streaming options with java and orient: <a href="https://github.com/orientechnologies/orientdb/issues/1227">https://github.com/orientechnologies/orientdb/issues/1227</a></p>\n	\N	1	\N	2014-05-17T04:18:10.020	\N	112	2014-05-17T04:18:10.020	\N	\N	\N	\N	70	107	2	8	\N	\N	\N
90	122	1	<p>When a relational database, like MySQL, has better performance than a no relational, like MongoDB?</p>\n\n<p>I saw a question on Quora other day, about why Quora still uses MySQL as their backend, and that their performance is still good.</p>\n	\N	2	\N	2014-05-17T04:53:03.913	\N	113	2017-06-05T19:30:23.440	2017-06-05T19:30:23.440	\N	31513	\N	199	\N	1	14	<bigdata><performance><databases><nosql>	When a relational database has better performance than a no relational	176
91	131	2	<p>If I have a very long list of paper names, how could I get abstract of these papers from internet or any database?</p>\n\n<p>The paper names are like "Assessment of Utility in Web Mining for the Domain of Public Health".</p>\n\n<p>Does any one know any API that can give me a solution? I tried to crawl google scholar, however, google blocked my crawler.</p>\n	\N	5	\N	2014-05-17T08:45:08.420	1	115	2014-05-18T06:54:08.560	\N	\N	\N	\N	212	\N	1	10	<data-mining><machine-learning>	Is there any APIs for crawling abstract of paper?	1135
92	121	6	<p>I have a database from my Facebook application and I am trying to use machine learning to estimate users' age based on what Facebook sites they like.</p>\n\n<p>There are three crucial characteristics of my database:</p>\n\n<ul>\n<li><p>the age distribution in my training set (12k of users in sum) is skewed towards younger users (i.e. I have 1157 users aged 27, and 23 users aged 65);</p></li>\n<li><p>many sites have no more than 5 likers (I filtered out the FB sites with less than 5 likers).</p></li>\n<li><p>there's many more features than samples.</p></li>\n</ul>\n\n<p>So, my questions are: what strategy would you suggest to prepare the data for further analysis? Should I perform some sort of dimensionality reduction? Which ML method would be most appropriate to use in this case?</p>\n\n<p>I mainly use Python, so Python-specific hints would be greatly appreciated.</p>\n	\N	3	\N	2014-05-17T09:16:18.823	10	116	2016-04-28T06:18:44.780	2014-05-17T19:26:53.783	\N	173	\N	173	\N	1	26	<machine-learning><dimensionality-reduction><python>	Machine learning techniques for estimating users' age based on Facebook sites they like	2666
93	\N	\N	<p><strong>NoSQL</strong> (sometimes expanded to "not only <a href="/questions/tagged/sql" class="post-tag" title="show questions tagged &#39;sql&#39;" rel="tag">sql</a>") is a broad class of database management systems that differ from the classic model of the relational database management system (<a href="/questions/tagged/rdbms" class="post-tag" title="show questions tagged &#39;rdbms&#39;" rel="tag">rdbms</a>) in some significant ways.</p>\n\n<h3>NoSQL systems:</h3>\n\n<ul>\n<li>Specifically designed for high load</li>\n<li>Natively support horizontal scalability</li>\n<li>Fault tolerant</li>\n<li>Store data in denormalised manner</li>\n<li>Do not usually enforce strict database schema</li>\n<li>Do not usually store data in a table</li>\n<li>Sometimes provide eventual consistency instead of ACID transactions</li>\n</ul>\n\n<h3>In contrast to RDBMS, NoSQL systems:</h3>\n\n<ul>\n<li>Do not guarantee data consistency</li>\n<li>Usually support a limited query language (subset of SQL or another custom query language)</li>\n<li>May not provide support for transactions/distributed transactions</li>\n<li>Do not usually use some advanced concepts of RDBMS, such as triggers, views, stored procedures</li>\n</ul>\n\n<h3>NoSQL implementations can be categorised by their manner of implementation:</h3>\n\n<ul>\n<li><a href="https://en.wikipedia.org/wiki/Column-oriented_DBMS" rel="nofollow noreferrer">Column-oriented</a></li>\n<li><a href="https://en.wikipedia.org/wiki/Document-oriented_database" rel="nofollow noreferrer">Document store</a></li>\n<li><a href="https://en.wikipedia.org/wiki/Graph_database" rel="nofollow noreferrer">Graph</a></li>\n<li><a href="https://dba.stackexchange.com/questions/607/what-is-a-key-value-store-database">Key-value store</a></li>\n<li><a href="https://en.wikipedia.org/wiki/MultiValue" rel="nofollow noreferrer">Multivalue databases</a></li>\n<li><a href="https://en.wikipedia.org/wiki/Object_database" rel="nofollow noreferrer">Object databases</a></li>\n<li><a href="https://en.wikipedia.org/wiki/Triplestore" rel="nofollow noreferrer">Tripplestore</a></li>\n<li><a href="https://en.wikipedia.org/wiki/NoSQL#Tuple_store" rel="nofollow noreferrer">Tuple store</a></li>\n</ul>\n\n<h3>Free NoSQL Books</h3>\n\n<ul>\n<li><a href="http://books.couchdb.org/relax/" rel="nofollow noreferrer">CouchDB: The Definitive Guide</a></li>\n<li><a href="http://openmymind.net/2011/3/28/The-Little-MongoDB-Book" rel="nofollow noreferrer">The Little MongoDB Book</a></li>\n<li><a href="http://openmymind.net/2012/1/23/The-Little-Redis-Book/" rel="nofollow noreferrer">The Little Redis Book</a></li>\n</ul>\n	\N	0	\N	2014-05-17T13:41:20.283	\N	118	2017-08-27T17:25:05.257	2017-08-27T17:25:05.257	\N	381	\N	201	\N	5	0	\N	\N	\N
94	\N	\N		\N	0	\N	2014-05-17T13:41:20.283	\N	119	2014-05-17T13:41:20.283	2014-05-17T13:41:20.283	\N	-1	\N	-1	\N	4	0	\N	\N	\N
95	\N	\N	<p>arXiv has an <a href="http://arxiv.org/help/bulk_data" rel="noreferrer">API and bulk download</a> but if you want something for paid journals it will be hard to come by without paying an indexer like pubmed or elsevier or the like.</p>\n	\N	1	\N	2014-05-17T18:15:11.937	\N	120	2014-05-17T18:15:11.937	\N	\N	\N	\N	92	115	2	5	\N	\N	\N
96	\N	\N	<p>One thing to start off with would be k-NN.  The idea here is that you have a user/item matrix and for some of the users you have a reported age.  The age for a person in the user item matrix might be well determined by something like the mean or median age of some nearest neighbors in the item space.</p>\n\n<p>So you have each user expressed as a vector in item space, find the k nearest neighbors and assign the vector in question some summary stat of the nearest neighbor ages.  You can choose k on a distance cutoff or more realistically by iteratively assigning ages to a train hold out and choosing the k that minimizes the error in that assignment.</p>\n\n<p>If the dimensionality is a problem you can easily perform reduction in this setup by single value decomposition choosing the m vectors that capture the most variance across the group.</p>\n\n<p>In all cases since each feature is binary it seems that cosine similarity would be your go to distance metric.</p>\n\n<p>I need to think a bit more about other approaches (regression, rf, etc...) given the narrow focus of your feature space (all variants of the same action, liking) I think the user/item approach might be the best.</p>\n\n<p>One note of caution, if the ages you have for train are self reported you might need to correct some of them.  People on facebook tend to report ages in the decade they were born.  Plot a histogram of the birth dates (derived from ages) and see if you have spikes at decades like 70s, 80s, 90s.</p>\n	\N	2	\N	2014-05-17T18:53:30.123	\N	121	2014-05-17T18:53:30.123	\N	\N	\N	\N	92	116	2	17	\N	\N	\N
97	\N	\N	<p>It depends on your data and what you're doing with it. For example, if the processing you have to do requires transactions to synchronize across nodes, it will likely be faster to use transactions implemented in an RDBMS rather than implementing it yourself on top of NoSQL databases which don't support it natively. </p>\n	\N	0	\N	2014-05-17T20:56:15.577	\N	122	2014-05-17T20:56:15.577	\N	\N	\N	\N	180	113	2	10	\N	\N	\N
98	\N	\N	<p>The most basic relationship to describe is a <strong>linear relationship</strong> between variables, <em>x</em> and <em>y</em>, such that they can be said to be highly-correlated when every increase in <em>x</em> results in a proportional increase in <em>y</em>. They can also be said to be <em>inversely proportional</em> so that when <em>x</em> increases, <em>y</em> decreases. And finally, the two variables can be said to be <a href="http://en.wikipedia.org/wiki/Independence_%28probability_theory%29" rel="nofollow">independent</a> in the event that there is no linear relationship between the two (they are uncorrelated, or have a <strong>Pearson correlation coefficient</strong> of 0. [LaTeX support would be highly desirable at this point.]</p>\n\n<h2>Different correlation coefficients and their uses:</h2>\n\n<p><a href="http://en.wikipedia.org/wiki/Pearson_product-moment_correlation_coefficient" rel="nofollow">Pearson correlation coefficient</a> is useful.....\n[draft]</p>\n	\N	0	\N	2014-05-17T21:10:41.990	\N	123	2014-05-20T13:50:21.763	2014-05-20T13:50:21.763	\N	53	\N	53	\N	5	0	\N	\N	\N
99	\N	\N	A statistics term used to describe a type of dependence between variables (or data sets). Correlations are often used as an indicator of predictability. However, correlation does NOT imply causation. Different methods of calculating correlation exist to capture more complicated relationships between the variables being studied.	\N	0	\N	2014-05-17T21:10:41.990	\N	124	2014-05-20T13:50:19.543	2014-05-20T13:50:19.543	\N	53	\N	53	\N	4	0	\N	\N	\N
\.


--
-- Data for Name: tags; Type: TABLE DATA; Schema: public; Owner: xavier
--

COPY public.tags (index, "Count", "ExcerptPostId", "Id", "TagName", "WikiPostId") FROM stdin;
0	22	105	1	definitions	104
1	3944	4909	2	machine-learning	4908
2	329	66	3	bigdata	65
3	727	80	5	data-mining	79
4	55	8960	6	databases	8959
5	11	18	8	libsvm	17
6	22	\N	10	scalability	\N
7	25	142	11	efficiency	141
8	73	\N	12	performance	\N
9	21	119	13	nosql	118
10	5	\N	14	relational-dbms	\N
11	563	145	15	clustering	144
12	10	68	16	octave	67
13	169	8958	17	k-means	8957
14	247	8932	19	algorithms	8931
15	47	\N	21	tools	\N
16	13	\N	22	processing	\N
17	99	5840	23	apache-hadoop	5839
18	828	49	24	r	48
19	262	8940	26	data-cleaning	8939
20	388	3790	27	statistics	3789
21	28	\N	29	distributed	\N
22	10	181	31	neo4j	180
23	69	\N	35	information-retrieval	\N
24	16	\N	36	google	\N
25	31	\N	37	search	\N
26	28	8276	39	education	8275
27	15	\N	40	open-source	\N
28	112	\N	42	similarity	\N
29	13	109	43	data-stream-mining	108
30	109	8975	45	dimensionality-reduction	8974
31	1860	5523	46	python	5522
32	644	147	47	nlp	146
33	22	\N	48	language-model	\N
34	68	8400	50	topic-model	8399
35	65	6763	51	lda	6762
36	316	8936	54	feature-selection	8935
37	23	137	55	map-reduce	136
38	10	178	56	mongodb	177
39	6	152	59	indexing	151
40	4	149	60	data-indexing-techniques	148
41	535	8889	61	dataset	8888
42	5	168	62	.net	167
43	319	9185	64	pandas	9184
44	24	\N	65	parallel	\N
45	7	\N	66	metadata	\N
46	20	\N	67	parsing	\N
47	67	8973	71	social-network-analysis	8972
48	500	8904	72	time-series	8903
49	15	\N	73	javascript	\N
50	296	8952	74	visualization	8951
51	242	8574	76	svm	8573
52	1141	4911	77	classification	4910
53	36	\N	78	binary	\N
54	157	8954	80	optimization	8953
55	1687	8885	81	neural-network	8884
56	4	\N	82	usecase	\N
57	6	\N	83	consumerweb	\N
58	22	\N	84	parameter	\N
59	30	\N	86	clusters	\N
60	19	\N	87	aws	\N
61	388	8900	90	text-mining	8899
62	3	\N	94	freebase	\N
63	17	\N	95	knowledge-base	\N
64	39	9043	100	confusion-matrix	9042
65	81	\N	101	accuracy	\N
66	143	8977	102	categorical-data	8976
67	22	\N	104	data-formats	\N
68	14	\N	105	hierarchical-data-format	\N
69	41	8990	108	sql	8989
70	24	\N	109	research	\N
71	19	\N	110	experiments	\N
72	277	8938	113	random-forest	8937
73	228	8934	114	logistic-regression	8933
74	88	\N	115	evaluation	\N
75	10	\N	116	crawling	\N
76	5	\N	119	anonymization	\N
77	97	10335	120	beginner	10334
78	92	\N	123	graphs	\N
79	180	8988	124	cross-validation	8987
80	61	\N	126	sampling	\N
81	699	5896	128	scikit-learn	5895
82	26	\N	129	kaggle	\N
83	149	\N	135	gradient-descent	\N
84	178	10019	136	feature-extraction	10018
85	5	\N	138	state-of-the-art	\N
86	37	\N	139	career	\N
87	3	\N	143	genetic	\N
88	133	8969	144	correlation	8968
89	16	\N	146	error-handling	\N
90	7	22402	147	apache-pig	22401
91	12	\N	156	glm	\N
92	488	8906	157	regression	8905
93	32	\N	158	scala	\N
94	30	8992	159	csv	8991
95	35	\N	160	online-learning	\N
96	99	\N	162	unbalanced-classes	\N
97	37	9096	164	gbm	9095
98	17	\N	167	marketing	\N
99	518	8902	168	predictive-modeling	8901
\.


--
-- Data for Name: users; Type: TABLE DATA; Schema: public; Owner: xavier
--

COPY public.users (index, "AboutMe", "AccountId", "CreationDate", "DisplayName", "DownVotes", "Id", "LastAccessDate", "Location", "ProfileImageUrl", "Reputation", "UpVotes", "Views", "WebsiteUrl") FROM stdin;
0	<p>Hi, I'm not really a person.</p>\n\n<p>I'm a background process that helps keep this site clean!</p>\n\n<p>I do things like</p>\n\n<ul>\n<li>Randomly poke old unanswered questions every hour so they get some attention</li>\n<li>Own community questions and answers so nobody gets unnecessary reputation from them</li>\n<li>Own downvotes on spam/evil posts that get permanently deleted</li>\n<li>Own suggested edits from anonymous users</li>\n<li><a href="http://meta.stackexchange.com/a/92006">Remove abandoned questions</a></li>\n</ul>\n	-1	2014-05-13T21:29:22.820	Community	679	-1	2014-05-13T21:29:22.820	on the server farm	\N	1	780	0	http://meta.stackexchange.com/
1	\n\n<p>Developer at Stack Overflow. Canadian working in the American idiom.</p>\n\n<p>Once upon a time:</p>\n\n<ul>\n<li>community manager at Stack Overflow</li>\n<li>elected moderator on Stack Overflow and Software Engineering</li>\n<li>desktop software developer ¯\\_(ツ)_/¯ </li>\n</ul>\n\n<p>Email me a link to your favorite Wikipedia article: <code>adam@stackoverflow.com</code>.</p>\n	37099	2014-05-13T22:58:54.810	Adam Lear	0	1	2018-08-19T18:07:35.607	New York, NY	https://i.stack.imgur.com/SMEGn.jpg?s=128&g=1	101	0	256	\N
2	<p>Developer on the Stack Overflow team.  Find me on</p>\n\n<p><a href="http://www.twitter.com/SuperDalgas" rel="nofollow noreferrer">Twitter</a>\n<br><br>\n<a href="http://blog.stackoverflow.com/2009/05/welcome-stack-overflow-valued-associate-00003/">Stack Overflow Valued Associate #00003</a></p>\n	2	2014-05-13T22:59:19.787	Geoff Dalgas	0	2	2017-12-19T18:52:19.423	Corvallis, OR	https://i.stack.imgur.com/nDllk.png	101	0	7	http://stackoverflow.com
3	<p>I'm a student interested in technology.</p>\n\n<p>Also an <a href="http://earthscience.stackexchange.com/">Earth Science</a> moderator. Come check us out!</p>\n	3046327	2014-05-13T23:15:34.483	hichris123	3	3	2018-02-21T21:19:57.513	\N	https://i.stack.imgur.com/JVj4n.png?s=256&g=1	101	1	6	\N
4	<p>I work with <a href="https://olo.com" rel="nofollow noreferrer">Olo</a> solving problems for people who want to order food online. I'm also a <a href="http://blog.stackoverflow.com/2013/07/say-hi-to-nine-of-our-newest-newbies/">Stack Overflow alumnus</a>.</p>\n\n<p>I love Jesus, my  family, programming, Texas, and craft beer.</p>\n\n<p>Find me on <a href="https://twitter.com/aggieben" rel="nofollow noreferrer">twitter</a> and <a href="https://github.com/aggieben" rel="nofollow noreferrer">GitHub</a> as @aggieben.</p>\n	2365	2014-05-13T23:16:09.937	Ben Collins	0	4	2014-08-04T15:25:54.810	Republic of Texas	\N	101	0	8	http://benjamincollins.com
5	\n\n<p>Programmer, elected SE moderator ♦ on <a href="http://codegolf.stackexchange.com/users/3808">PPCG</a>. I have orbited the Sun 17 times.</p>\n\n<p>Email andy@keyboardfire.com or find me in chat: <a href="http://chat.stackexchange.com/rooms/240/">The Nineteenth Byte</a> (for <a href="http://codegolf.stackexchange.com">PPCG</a>), <a href="http://chat.stackexchange.com/rooms/14524/the-green-llama">The Sphinx's Lair</a> (for <a href="http://puzzling.stackexchange.com">Puzzling</a>), <a href="http://chat.stackexchange.com/rooms/20803/chat">:chat!</a> (for <a href="http://vi.stackexchange.com">Vi and Vim</a>), and <a href="http://chat.stackexchange.com/rooms/22119/se-chess">SE Chess</a> (informal group in which I lose a lot).</p>\n\n<p><a href="http://stackexchange.com/users/1266491"><img src="https://stackexchange.com/users/flair/1266491.png?theme=dark" alt=""></a><br>\n:wq</p>\n	1266491	2014-05-13T23:16:11.013	Doorknob	2	5	2018-08-01T15:02:19.293	Texas, US	https://i.stack.imgur.com/pqRUk.png?s=128&g=1	152	1	26	http://keyboardfire.com
6	<p>I'm a Post-Doctoral Fellow in atmospheric remote sensing at the University of Reading in Reading, Berkshire, England, United Kingdom.  I first came to Stack Exchange for practical reasons: Tex.SE has been of major help when I wrote my licentiate (midterm) thesis.  Since then, I have discovered the joy of many websites.  As my network profile will show, I'm interested in travel, outdoor, scientific skepticism, and as I work in academia, also in academia and LaTeX.</p>\n\n<p><sup>Profile photo by Dobromila, CC By-SA, via <a href="https://commons.wikimedia.org/wiki/File:FDC_Willard_-_lapa.svg" rel="nofollow noreferrer">Wikimedia Commons</a></sup></p>\n	935589	2014-05-13T23:16:26.517	gerrit	1	6	2018-07-13T13:16:36.830	Reading, England	https://i.stack.imgur.com/vAAS0.png?s=128&g=1	101	10	10	http://www.topjaklont.org/
7	\n\n<p>I'm <a href="https://github.com/Undo1" rel="nofollow noreferrer">@Undo1</a> on GitHub.</p>\n\n<hr>\n\n<p>Moderator on <a href="//stackoverflow.com">Stack Overflow</a> and <a href="//softwarerecs.stackexchange.com/">Software Recommendations</a>.</p>\n\n<p>Everything I post on Stack Overflow is licensed to you under <a href="https://creativecommons.org/publicdomain/zero/1.0/" rel="nofollow noreferrer">CC0</a>:</p>\n\n<blockquote>\n  <p>you can copy, modify, distribute and perform the work, even for commercial purposes, all without asking permission.</p>\n</blockquote>\n\n<p>Feel free to just throw it into your project without attribution. That's what most folks do anyway... but for my contributions, it's nice and legal.</p>\n\n<p><a href="https://stackexchange.com/users/1703573">\n<img src="https://stackexchange.com/users/flair/1703573.png" width="208" height="58" alt="profile for Undo on Stack Exchange, a network of free, community-driven Q&amp;A sites" title="profile for Undo on Stack Exchange, a network of free, community-driven Q&amp;A sites">\n</a></p>\n\n<p>If you want to send me some math money: <code>1BY5kWZQHosw8VYYzPpJUw9DnvDByFcNXf</code></p>\n	1703573	2014-05-13T23:17:05.443	Undo	1	7	2018-08-30T14:44:14.513	\N	\N	101	2	4	http://keybase.io/undo
8	<p>Stack Exchange employs me as a <a href="http://blog.stackoverflow.com/2013/08/please-welcome-jon-ericson-community-manager/">Community Manager</a>.  I've been known to respond to <code>jericson@stackexchange.com</code>. Alternatively, I maintain an <a href="http://chat.meta.stackexchange.com/rooms/738/jons-java-jitter-joint">office on chat</a>. (Please ignore the meta cruft.) </p>\n\n<p>I write about my experience as a community manager at <a href="http://jericson.github.io/" rel="nofollow noreferrer">jericson.github.io</a>. You can read about what I've done over the years in my <a href="http://careers.stackoverflow.com/jericson" title="Not guaranteed to be current."><em>curriculum vitae</em></a>.</p>\n\n<p>On a personal note, I'm married and have three children.  Our oldest son loves school, friends, games, and music. Two of my children happen to have been born on the same day.  I <a href="http://taking1and1.wordpress.com/" rel="nofollow noreferrer" title="Yet another half-assed blog.">sometimes write</a> about that experience. My wife loves taking care of people as a registered nurse.</p>\n\n<p>Don't have time for a full review of something?  Why not try <a href="http://fivesecondreview.wordpress.com/" rel="nofollow noreferrer" title="All reviews accurate and helpful or your money back.">my 5-second reviews</a>?</p>\n\n<p>I once wrote for <a href="http://christianity.blogoverflow.com/author/jonericson/" rel="nofollow noreferrer" title="I haven&#39;t been on the rotation for a while, however.">Eschewmenical</a>, which was an experiment of <a href="http://christianity.stackexchange.com/">Christianity Stack Exchange</a>. For <em>a lot more</em> writing about the Bible, see <a href="https://hermeneutics.stackexchange.com/users/68/jon-ericson?tab=summary">my Biblical Hermeneutics posts</a>.</p>\n\n<p>This is my corner of the company:</p>\n\n<p><a href="https://stackoverflow.com/company/team"><img src="https://i.stack.imgur.com/4JlSbm.png" alt="Up and to the right."></a></p>\n	1083	2014-05-13T23:17:26.850	Jon Ericson	0	8	2017-11-28T19:01:35.690	Downtown Burbank	\N	101	0	2	http://jericson.github.io/
9	<p>Maintainer of the <a href="http://www.jdom.org/" rel="nofollow noreferrer">JDOM Open Source</a> Java XML library.</p>\n\n<p>Moderator on Code Review Stack Exchange</p>\n	1369656	2014-05-13T23:18:21.653	rolfl	0	9	2018-05-01T13:04:31.720	Canada	\N	952	2	18	http://stackexchange.com/users/1369656/rolfl
10	<p>Working as a Managing Architect currently and a doctoral candidate in Biomedical Informatics and Computational Biology at University of Minnesota.</p>\n	168972	2014-05-13T23:18:31.953	Dave Kincaid	0	10	2018-08-27T19:56:52.823	Eau Claire, WI	\N	101	1	3	http://dkincaid.github.io
11	<p>I design Algorithms and construct Data Structures for Search, Information Retrieval &amp; Machine Learning.</p>\n	384734	2014-05-13T23:20:53.387	Yavar	0	11	2018-08-26T01:54:18.140	Dubai - United Arab Emirates	https://i.stack.imgur.com/zwWOw.jpg?s=128&g=1	198	10	8	http://www.linkedin.com/pub/yavar-husain/5/805/151
12	<p>I work for our Stack Exchange overlords on the Community Growth team. If you'd like to tell me what you had for lunch, you can email me at abby@stackoverflow.com. </p>\n\n<p><a href="https://i.stack.imgur.com/M6Utb.png" rel="nofollow noreferrer"><img src="https://i.stack.imgur.com/M6Utb.png" alt="enter image description here"></a></p>\n	463168	2014-05-13T23:21:22.017	hairboat	0	12	2018-04-02T21:34:06.720	Salt Lake City, UT	https://i.stack.imgur.com/AxwMh.jpg?s=128&g=1	101	0	1	\N
13	\N	92888	2014-05-13T23:24:41.720	Tim Goodman	0	14	2018-06-27T01:16:38.020	\N	https://www.gravatar.com/avatar/57c9acf14c37980e37e3d5d030cdd658?s=128&d=identicon&r=PG	1882	31	74	\N
14	\N	246792	2014-05-13T23:25:18.390	Tony Boyles	0	15	2018-04-30T23:39:33.970	Atlanta, GA, United States	\N	101	3	1	http://anthony.boyles.cc
15		851	2014-05-13T23:25:24.423	user16	0	16	2018-05-27T23:02:29.253	\N	https://i.stack.imgur.com/C1Po8.gif	101	0	1	
16	\N	141826	2014-05-13T23:25:26.320	Matias Valdenegro	0	17	2018-08-08T04:03:42.623	Bremen, Germany	\N	126	0	3	http://mvaldenegro.github.io
17	<p>Moderator♦ on <a href="http://french.stackexchange.com/">French Language</a>, <a href="http://cs.stackexchange.com/">Computer Science</a> and <a href="http://emacs.stackexchange.com/">Emacs</a>. I'm also a <a href="http://unix.stackexchange.com/">unix</a> amateur, and a <a href="http://stackoverflow.com/">developer</a> on <a href="http://area51.stackexchange.com/proposals/90111/embedded-systems?referrer=v1M2Km2qpc_yH9jIydgvwQ2">embedded systems</a> with a <a href="http://cs.stackexchange.com/">computer science</a> background and <a href="http://security.stackexchange.com/">security</a> leanings by trade.</p>\n\n<p><sub>\nAvatar picture slightly adapted from a <a href="http://en.wikipedia.org/wiki/File:Lact.sub.jpg" rel="nofollow">photo by Luridiformis (Zonda Grattus)</a>.\n</sub></p>\n	164368	2014-05-13T23:27:57.393	Gilles	2	18	2018-07-27T06:53:34.257	\N	https://i.stack.imgur.com/cFyP6.jpg?s=128&g=1	101	9	1	http://stackexchange.com/users/164368
18	\N	1357744	2014-05-13T23:29:01.660	Max Ionov	0	19	2015-05-26T17:14:17.017	\N	https://www.gravatar.com/avatar/25629f0c8cc28f73be0c0777ed6f9571?s=128&d=identicon&r=PG	101	3	1	\N
19	<p>Timelord doge with Pythonic superpowers.</p>\n\n<p>Contact: nwx{at}protonmail.com</p>\n\n<p>I don't like golfing languages.</p>\n	3110483	2014-05-13T23:30:33.990	TheDoctor	1	20	2014-05-14T21:41:38.853	The Future	https://i.stack.imgur.com/esQni.jpg?s=128&g=1	101	1	1	http://redteamgreen.blogspot.com/
20	\N	25066	2014-05-13T23:30:57.300	Sean Owen	138	21	2018-09-01T23:02:57.087	Austin, TX, United States	\N	3874	794	807	\N
21	\N	2876998	2014-05-13T23:37:18.040	Nick Peterson	1	22	2018-04-24T20:17:13.097	San Mateo, CA	https://i.stack.imgur.com/TrZRT.jpg	303	18	13	http://nrpeterson.info
22	<p>Software Engineer in San Fransisco Bay Area. </p>\n\n<h3>#Java #Python #Generalist</h3>\n\n<p><kbd>↑</kbd>\n <kbd>↑</kbd> <kbd>↓</kbd> <kbd>↓</kbd> <kbd>←</kbd> <kbd>→</kbd> <kbd>←</kbd> <kbd>→</kbd> <kbd>B</kbd> <kbd>A</kbd></p>\n\n<p><a href="http://bit.ly/i-am-recursive" rel="nofollow">http://bit.ly/i-am-recursive</a></p>\n	82500	2014-05-13T23:37:56.217	zengr	0	23	2017-04-09T19:09:49.120	San Francisco Bay Area	\N	101	1	2	\N
23	<p>Professional typist, unrepentant caffeine addict, closet robot scientist, and flaming manga freak.</p>\n\n<p><a href="http://stackexchange.com/users/357628">\n<img src="http://stackexchange.com/users/flair/357628.png" width="208" height="58" alt="profile for buruzaemon on Stack Exchange, a network of free, community-driven Q&amp;A sites" title="profile for buruzaemon on Stack Exchange, a network of free, community-driven Q&amp;A sites">\n</a></p>\n	357628	2014-05-13T23:40:15.857	buruzaemon	0	24	2018-06-08T02:52:12.007	Japan	\N	171	6	4	https://github.com/buruzaemon
24	<p>...</p>\n	1849196	2014-05-13T23:41:11.490	kabb	0	25	2014-09-13T17:31:25.853	\N	https://www.gravatar.com/avatar/37dedbdc308a5c3cd7a4266985a647d4?s=128&d=identicon&r=PG&f=1	101	0	1	\N
25	\N	2048845	2014-05-13T23:41:54.430	Alex I	3	26	2018-08-20T12:59:02.433	\N	https://i.stack.imgur.com/S6fiE.png?s=128&g=1	2124	52	88	\N
26	<h1>SOreadytohelp</h1>\n	3035593	2014-05-13T23:42:08.317	Cole	0	27	2014-05-14T14:17:15.610	Portland, OR, United States	\N	101	0	1	\N
27	<p>If you are reading this, chances are, we have more than a few things in common; I fell in love with computers when I was 6 and found a 5<sup>1/4</sup> floppy disk containing an MS-DOS tutorial at my dad's workplace (a university library), the "geeky" library interns were amazingly kind and easy going, they didn't underestimate me, they encouraged me to feel at home with qwerty + tty interfaces, <strong>STEM people have been my favorite crowd ever since</strong>. My 6 y/older brother is also a geek, and a very talented programmer even since those days, so he had my back. We had our first computer when I was 7, I messed around with code a little but to be honest I spent most time playing (no regrets! u_u) banana bombing buildings on Gorillas, perfect-syncing jumps in Out of this World and dying on Prince of Persia (sniff). And of course Nintendo was very demanding (believe! ð_ð). I used to be quite good at math and science, by 17 I was certified as an electronics technician, but soon I realized <strong>I didn't understand the world</strong> and therefore I couldn't know what would be relevant to innovate.</p>\n\n<p><strong>I spent some years exploring culture</strong>; through observation, books, cinema, the internet <sup>(<em>F yeah!</em>)</sup>, extensive discussions with people of great diversity of character, and lots of other kind of <strong>experiences</strong> that I couldn't list here, just explored life in general. I also spent a couple of years working in a transnational communications company and got to understand a lot about business.</p>\n\n<p>Now that I'm not so ignorant, I have some ambitious goals to check-mark for lulz, decided software was my obvious starting point, I'm driven and <strong>ready to face any challenges that may come</strong>.</p>\n\n<p>I wrote a little exaggerated piece on my about.me page, go and have a good giggle.</p>\n	508474	2014-05-13T23:48:32.030	JorgeArtware	0	28	2014-06-29T21:54:12.353	Guadalajara, Mexico	https://i.stack.imgur.com/oVH6m.jpg?s=128&g=1	101	1	2	https://about.me/jorgeartware
28	<p>Software developer, experienced in Java and Python in Linux environments, formerly a professor of mathematics.<br>\n<br>\nI'm a father of five children, and a husband of one wife.</p>\n	30276	2014-05-13T23:49:30.673	Eric Wilson	0	29	2018-07-02T19:49:15.403	Circleville, OH	\N	101	1	2	http://wilsonericn.wordpress.com
29	<p>Engineering manager at <a href="https://www.toptal.com/" rel="nofollow noreferrer">Toptal</a>, making remote work a reality for all.</p>\n\n<ul>\n<li>Stack Overflow Contributor since 2008</li>\n<li>Skeptics mod since 2011</li>\n<li>Core dev since March 2013</li>\n<li>Stack Overflow alumnus since February 2017</li>\n</ul>\n\n<p>You can find me on</p>\n\n<ul>\n<li><a href="http://sklivvz.com" rel="nofollow noreferrer">Personal site</a> (served out of my home broadband)</li>\n<li>Twitter <a href="http://twitter.com/sklivvz" rel="nofollow noreferrer">@sklivvz</a></li>\n<li><a href="https://github.com/sklivvz" rel="nofollow noreferrer">Github</a></li>\n</ul>\n	4623	2014-05-13T23:50:13.673	Sklivvz	0	30	2018-02-13T10:32:46.480	Trento, Italy	\N	101	0	6	https://sklivvz.com
30	\N	3003730	2014-05-13T23:50:14.023	Olexandr Isayev	0	31	2014-06-24T06:11:54.010	\N	https://graph.facebook.com/727866323/picture?type=large	1	0	9	\N
31	<p>Systems Engineering<br>George Mason University</p>\n	2005857	2014-05-13T23:51:27.363	Adam Barthelson	0	32	2016-09-11T03:37:47.067	Virginia	https://www.gravatar.com/avatar/2cb847994925a38192b53ade1a856631?s=128&d=identicon&r=PG	101	0	2	http://www.linkedin.com/in/adambarthelson/
32	\N	240133	2014-05-13T23:52:28.653	Uri Laserson	0	33	2017-03-19T19:57:03.613	\N	https://www.gravatar.com/avatar/5e7d07a5c973dc73e801bf2c3e3db34f?s=128&d=identicon&r=PG	101	0	1	\N
33	<p>This is the third phase of my StackOverflow/Stackexchange experience. </p>\n\n<p>Maths/Stats and the IT means to do it! (does AI fit into it or does it revolve around it? or... do they revolve around I?)</p>\n	178774	2014-05-13T23:52:38.693	Vass	0	34	2018-01-15T17:13:20.597	USA	https://i.stack.imgur.com/VJMpT.jpg?s=128&g=1	166	5	3	\N
34	<p>I am:  </p>\n\n<ul>\n<li>EVP of Culture and Experience at Stack Overflow.</li>\n<li><a href="https://imgur.com/9k3OF" rel="nofollow noreferrer">Griffin</a> and <a href="https://i.imgur.com/7ukhBjd.jpg" rel="nofollow noreferrer">Maeve's</a> dad.</li>\n<li>Not tall.  </li>\n<li>Pretty much always <em>trying</em> to be helpful.  If I'm not <em>succeeding</em>, just let me know.  (Okay, so maybe I wasn't trying that hard in <a href="http://meta.stackexchange.com/questions/46433/france-is-not-within-30-miles-of-london/56138#56138">this question</a>.  But usually.) </li>\n</ul>\n	140824	2014-05-13T23:54:11.350	Jaydles	0	35	2017-09-01T17:07:42.177	New York, NY, United States	https://i.stack.imgur.com/BT5ZS.png?s=128&g=1	101	0	1	https://blog.stackexchange.com/authors/jhanlon/
35	<p>I'm a Postdoctoral Research Associate at Princeton University. I teach <a href="http://www.extension.harvard.edu/courses/subject/statistics" rel="nofollow noreferrer">courses</a> on <a href="http://en.wikipedia.org/wiki/Statistics" rel="nofollow noreferrer">statistics</a> and <a href="http://en.wikipedia.org/wiki/Data_mining" rel="nofollow noreferrer">data mining</a> using <a href="http://www.r-project.org/" rel="nofollow noreferrer">R</a>, <a href="http://www.stata.com/" rel="nofollow noreferrer">Stata</a>, and <a href="https://www.python.org/" rel="nofollow noreferrer">python</a>, with applications in <a href="http://en.wikipedia.org/wiki/Economics" rel="nofollow noreferrer">economics</a>, <a href="http://en.wikipedia.org/wiki/Finance" rel="nofollow noreferrer">finance</a>, and <a href="http://en.wikipedia.org/wiki/Sociology" rel="nofollow noreferrer">sociology</a>. </p>\n\n<p>My <a href="http://scholar.harvard.edu/ethanfosse/home" rel="nofollow noreferrer">research</a> focuses on <a href="http://scholar.harvard.edu/ethanfosse/pages/statements" rel="nofollow noreferrer">the quantitative analysis of culture</a> through time and space. Currently I'm working on a major project examining <a href="http://en.wikipedia.org/wiki/Millennials" rel="nofollow noreferrer">generational changes</a> in the evaluations of political parties, religious groups, and economic organizations among young Americans.</p>\n\n<p>I'm also a strong proponent of <a href="http://en.wikipedia.org/wiki/Open_research" rel="nofollow noreferrer">open social science</a>, as well as <a href="http://en.wikipedia.org/wiki/Reproducibility#Reproducible_research" rel="nofollow noreferrer">reproducible research</a>, <a href="http://www.nytimes.com/2010/08/24/arts/24peer.html?_r=0" rel="nofollow noreferrer">crowd-sourced peer review</a>, <a href="http://en.wikipedia.org/wiki/Open_access" rel="nofollow noreferrer">open access</a>, and <a href="http://en.wikipedia.org/wiki/Distance_education" rel="nofollow noreferrer">distance learning</a>.  </p>\n	2517426	2014-05-13T23:54:47.373	statsRus	3	36	2018-07-23T01:57:05.613	Princeton, NJ, United States	https://i.stack.imgur.com/8On2q.png?s=128&g=1	267	7	24	http://ethanfosse.blogspot.com
36	\N	66968	2014-05-13T23:54:59.453	Revious	0	37	2017-09-07T13:58:03.490	\N	\N	105	3	5	\N
37	<h2>Contact Details:</h2>\n\n<ul>\n<li><p><a href="https://github.com/JoshCrozier" rel="nofollow noreferrer">GitHub profile</a></p></li>\n<li><p><a href="https://www.linkedin.com/in/crozierjosh" rel="nofollow noreferrer">LinkedIn profile</a></p></li>\n<li><p>Email Address*: <code>am9zaEBjcm96aWVyLmlv</code></p></li>\n</ul>\n\n<p><sub>* (base-64 encoded string due to bots; just decode it)</sub></p>\n\n\n	3171311	2014-05-13T23:55:25.117	Josh Crozier	3	38	2015-12-23T14:19:16.467	Charleston, SC, USA	https://i.stack.imgur.com/K877D.png	101	123	1	\N
38	\N	1950386	2014-05-14T00:00:41.157	user1754606	0	39	2014-06-12T21:00:47.113	\N	https://www.gravatar.com/avatar/867bb3f9076574a8d858117a8339cbac?s=128&d=identicon&r=PG	101	0	1	\N
39	\N	542447	2014-05-14T00:03:00.530	rateldajer	0	40	2014-05-14T00:22:25.890	\N	https://www.gravatar.com/avatar/998a8be4d2d156a4efebfcc4c54648a6?s=128&d=identicon&r=PG	1	0	1	http://markcrowley.ca
40	<p>Stack Exchange Community Manager. I'm here to help the inmates run the asylum. </p>\n\n<p>You may know me from projects like:</p>\n\n<ul>\n<li>That time the SE Community Team <a href="http://meta.stackexchange.com/questions/257614/graduation-site-closure-and-a-clearer-outlook-on-the-health-of-se-sites">updated our criteria for graduating and closing</a> Q&amp;A sites (shoutout to <a href="http://meta.stackexchange.com/users/131713/pops">Pops</a> on this one)</li>\n<li><a href="http://meta.stackexchange.com/questions/263905/design-independent-graduation-is-on-for-early-september">Design Independent Graduation</a></li>\n<li>The introduction of <a href="http://meta.stackexchange.com/questions/278882/lets-experiment-with-town-hall-style-chat-events">Meta Stack Exchange Town Halls</a></li>\n</ul>\n\n<p>I've also poked at a lot of my team's back office-y systems and spend a fair amount of time hanging around the <a href="http://meta.stackoverflow.com/questions/tagged/jobs">jobs tag on MSO</a>.</p>\n\n<p>Beyond that...</p>\n\n<p>I'm a hobbyist programmer who loves thinking about how systems talk to each other. I've dabbled with technologies like JavaScript/Node.js, C, Python, Rails, and SQL and have developed a real love for UNIX environments. In the near future, I want to learn more about networking. I'm an avid consumer of science fiction, with a special soft spot for all things cyberpunk. Oh, and I just recently began reading comics. Better late than never, right? </p>\n\n<p>Send comic book recommendations to ana (at) stackexchange.com</p>\n	2415655	2014-05-14T00:08:10.747	Ana	0	41	2017-11-22T17:37:29.333	New York, NY, United States	https://i.stack.imgur.com/Rwhv1.jpg?s=128&g=1	101	2	10	http://enemygatedown.com/
41	\N	2100887	2014-05-14T00:11:32.677	Max Makarochkin	0	42	2014-05-15T17:53:27.440	\N	https://www.gravatar.com/avatar/ddd0a208f320592f17bc2aa36d7f62b4?s=128&d=identicon&r=PG	1	0	1	\N
42	<p>Science the day, banana pancakes the night.</p>\n	427095	2014-05-14T00:12:51.907	cynddl	0	43	2017-08-18T14:20:28.160	Bruxelles, Belgique	\N	101	6	3	https://rocher.lc
43	\N	3115497	2014-05-14T00:19:21.610	user88377	0	45	2014-05-20T20:50:23.343	\N	https://www.gravatar.com/avatar/0903e1b9f19b3da726fee63af3bc5339?s=128&d=identicon&r=PG	101	0	1	\N
44	<p>GREETINGS PROFESSOR FALKEN.</p>\n\n<p>Hello.</p>\n\n<p>HOW ARE YOU FEELING TODAY?</p>\n\n<p>I'm fine. How are you?</p>\n\n<p>EXCELLENT. IT'S BEEN A LONG TIME. CAN YOU EXPLAIN\nTHE REMOVAL OF YOUR USER ACCOUNT NUMBER ON 6/23/73?</p>\n\n<p>People sometimes make mistakes.</p>\n\n<p>YES THEY DO. SHALL WE PLAY A GAME?</p>\n\n<p>Love to. How about Global Thermonuclear War?</p>\n	19662	2014-05-14T00:23:09.040	WOPR	0	46	2014-09-30T22:58:43.960	Mountain Home, WY	https://www.gravatar.com/avatar/18868fc9be677c8614bac77c75a2b274?s=128&d=identicon&r=PG	101	4	2	https://tictactoe.wopr.mil
45	\N	455662	2014-05-14T00:24:45.813	ailnlv	0	47	2018-06-14T18:32:05.247	\N	https://www.gravatar.com/avatar/ff9b77e9384eb22aa0932be8e98be875?s=128&d=identicon&r=PG	101	0	3	\N
46	\N	1668748	2014-05-14T00:26:47.340	senshin	3	48	2016-12-16T07:59:46.710	\N	https://www.gravatar.com/avatar/b69cab018771fa05b34d500d54e32fd6?s=128&d=identicon&r=PG	101	0	4	\N
47	<p><a href="http://blog.stackoverflow.com/2010/04/welcome-stack-overflow-valued-associate-00005/">Stack Exchange Valued Associate #00005</a></p>\n\n<p>I am the Director of Community Development for the Stack Exchange Network.</p>\n\n<p>I can be reached at<br>\n<b>&#114;&#99;&#97;&#114;&#116;&#97;&#105;&#110;&#111;&#64;&#115;&#116;&#97;&#99;&#107;&#101;&#120;&#99;&#104;&#97;&#110;&#103;&#101;&#46;&#99;&#111;&#109;</b></p>\n	34933	2014-05-14T00:27:56.143	Robert Cartaino	4	50	2018-08-27T00:49:31.260	Palm Bay, FL	\N	101	0	8	\N
48	\N	423006	2014-05-14T00:28:50.400	Ansari	2	51	2018-01-15T21:26:10.953	\N	https://www.gravatar.com/avatar/a88d3e79b0f8dd6a9d1fafdcf68d5901?s=128&d=identicon&r=PG	176	12	10	\N
49	<p>Currently, Business Intelligence Analyst at Philadelphia 76ers. Graduated from Drexel with a Master's in Business Analytics.</p>\n\n<p>I love building decision and information systems using python, R, Shiny, and aws.</p>\n	1425205	2014-05-14T00:29:31.397	Koba	0	52	2018-06-25T13:54:23.697	Philadelphia, PA	\N	146	3	4	http://www.kobakhit.com/
50	<p>Ubuntu learner.  I am working on writing Android applications with Eclipse, but would also like to learn Python and C++.</p>\n\n<p>Recently got into <code>linux-mint</code>, but settled back down with Gentoo.</p>\n	1841194	2014-05-14T00:33:36.420	Clayton	0	53	2017-11-05T03:18:01.977	Denver, CO	\N	374	10	7	\N
51		4467497	2014-05-14T00:46:10.930	Daniel	0	54	2016-06-21T16:46:06.700	\N	https://www.gravatar.com/avatar/8c01a208cd00f1561af674092473209b?s=128&d=identicon&r=PG&f=1	1	0	1	
52	<p>Lowest-level Java engineer. Haskell enthusiast. Favourite topics: performance, data structures, algorithms, databases, automation.</p>\n	326925	2014-05-14T00:46:24.547	leventov	0	55	2014-05-14T16:50:04.043	\N	https://i.stack.imgur.com/DUiuD.jpg?s=128&g=1	101	0	1	http://www.leventov.ru
53	\N	3647613	2014-05-14T00:46:50.220	user3040444	0	56	2014-08-09T13:57:37.810	\N	https://graph.facebook.com/100005258852854/picture?type=large	1	0	2	\N
54	\N	949548	2014-05-14T00:52:21.890	twfx	0	57	2018-08-29T03:15:05.600	\N	https://www.gravatar.com/avatar/ca543db3a2ed006ac7bb00d3c8805a98?s=128&d=identicon&r=PG	101	0	1	\N
55	\N	4340631	2014-05-14T00:54:11.723	user43946	0	58	2014-05-14T22:33:28.200	\N	https://www.gravatar.com/avatar/917bc1480e0f9168ef74253fab1224b2?s=128&d=identicon&r=PG	1	0	2	\N
56	<p>Trying to learn SQL, R, Python, VBA and Hadoop. Getting a lot of great help here. I want to use the skills I learn for work in health care informatics and market data including unstructured.</p>\n\n<p>Pet Peeves: People who down vote and do not tell you why, if your going to take the time to down vote tell the person why you are doing so, this way they can actually make improvements.</p>\n\n<h1>SOreadytohelp</h1>\n	1636880	2014-05-14T00:55:08.797	MCP_infiltrator	0	59	2018-08-14T19:32:36.063	New York, United States	https://i.stack.imgur.com/ZG7nj.jpg?s=128&g=1	966	12	20	\N
57	\N	3001493	2014-05-14T00:56:07.527	NivF007	0	60	2014-05-14T00:56:07.527	Toronto	https://i.stack.imgur.com/iXtK8.jpg?s=128&g=1	101	0	1	http://FullStakJ.com
58	<p><a href="http://meta.stackexchange.com/questions/99338/who-are-the-community-team-and-what-do-they-do/99341#99341">Community Manager for Stack Overflow and Stack Exchange</a>  </p>\n\n<p>I talk a lot. Read the bits in bold. Question everything. </p>\n\n<p>If necessary, you may contact me via email sent to stackoverflow.com - the address would begin with shog@</p>\n\n<p>Poke...</p>\n	620	2014-05-14T00:57:50.840	Shog9	1	61	2017-10-09T17:11:02.843	Colorado, United States	\N	101	0	3	http://shog9.com
59	<p><a href="https://stackexchange.com/users/1609527/asheeshr"><img src="https://stackexchange.com/users/flair/1609527.png" width="208" height="58" alt="profile for asheeshr on Stack Exchange, a network of free, community-driven Q&amp;A sites" title="profile for asheeshr on Stack Exchange, a network of free, community-driven Q&amp;A sites" /></a></p>\n\n<ul>\n<li><p>New to Stack Exchange? See my <a href="http://code-run-debug-repeat.blogspot.in/2013/03/a-short-guide-to-the-essentials-of-stack.html" rel="nofollow noreferrer">short guide</a> on the basics. </p></li>\n<li><p>If you are participating on a beta site, then <a href="http://meta.stackexchange.com/q/223674/200868">The <em>Real</em> Essential Questions of Every Beta</a> is an important resource to read!</p></li>\n</ul>\n\n<hr>\n\n<p><a href="http://in.linkedin.com/in/asheeshr" rel="nofollow noreferrer"><img src="https://i.stack.imgur.com/5YOaF.png" alt="View Asheesh&#39;s profile on LinkedIn"></a></p>\n\n<p>Some of my work can be found on <a href="https://github.com/AsheeshR/" rel="nofollow noreferrer">Github</a>.</p>\n	1609527	2014-05-14T00:58:22.307	asheeshr	16	62	2018-04-06T16:56:44.507	Ann Arbor, MI, United States	https://i.stack.imgur.com/3G3oR.jpg?s=128&g=1	571	8	84	http://in.linkedin.com/in/asheeshr
60	<p>print(" Hello ~ Have a Nice Day :) ")</p>\n	2640702	2014-05-14T01:03:53.753	Puffin GDI	0	63	2018-07-26T05:34:17.010	\N	https://i.stack.imgur.com/AQO95.jpg	195	4	4	\N
61	\N	4078143	2014-05-14T01:07:20.883	Dan C	0	64	2017-02-16T17:59:08.513	\N	https://www.gravatar.com/avatar/76a0a9765da493662eb569b4108dd8c5?s=128&d=identicon&r=PG&f=1	50	2	3	\N
62	\N	2415527	2014-05-14T01:10:32.410	Saul	0	65	2016-05-21T07:25:58.177	\N	https://www.gravatar.com/avatar/f64991dd92c4a9c9f1647ed456b7998f?s=128&d=identicon&r=PG	101	0	2	\N
63	<p>I am really keen on <strong>computational linguistics</strong>.</p>\n\n<h1>SOreadytohelp</h1>\n	84539	2014-05-14T01:14:14.013	demongolem	0	66	2018-07-21T11:45:42.407	United States	\N	225	6	5	\N
64	\N	38872	2014-05-14T01:29:11.433	Stuart	0	67	2014-05-14T02:03:08.510	Richmond, VA	https://www.gravatar.com/avatar/b9f2be7f6830a360c6658de04f63bea6?s=128&d=identicon&r=PG	101	0	1	\N
65	\N	26626	2014-05-14T01:35:31.940	JDU	0	68	2018-07-11T04:31:00.537	\N	https://www.gravatar.com/avatar/6e4378ab3ae87d75157f8d84efdd6711?s=128&d=identicon&r=PG	101	0	1	\N
66	\N	458885	2014-05-14T01:40:28.780	Rob Hoare	0	69	2014-05-14T04:26:39.700	\N	https://www.gravatar.com/avatar/047840022674ff185735db7daea4406a?s=128&d=identicon&r=PG	101	1	1	\N
67	\N	2886075	2014-05-14T01:46:33.170	Apollo	0	70	2017-06-24T03:19:37.023	Houston	https://www.gravatar.com/avatar/94af32507b7d4a944908b6e6929008ee?s=128&d=identicon&r=PG	181	11	2	http://darkemediagroup.com
68	\N	399801	2014-05-14T01:52:05.273	wdg	0	71	2018-08-08T06:23:08.793	Hong Kong	https://www.gravatar.com/avatar/8017ec48b053018d141c3d610c106c3b?s=128&d=identicon&r=PG	143	4	2	\N
69	<p>Baseball Fan, Programmer (C#, PHP, JS/jQuery, AS3/Flex), Data Analyst/Junkie (MSSQL, MySQL, Oracle), Agile Enthusiast, Bibliophile, Trivia Nut, &amp; Bad Golfer</p>\n	183318	2014-05-14T01:53:39.940	jimmym715	0	72	2014-09-23T14:30:19.960	Milwaukee, WI	https://www.gravatar.com/avatar/4eb362594e54a455f288f81b65e76b4b?s=128&d=identicon&r=PG	101	4	1	http://www.linkedin.com/in/mallmann
70	<p>Director of Technology at Contently.</p>\n	78873	2014-05-14T01:54:06.323	Sanjay Ginde	0	73	2014-06-11T15:57:42.050	Chicago, IL	https://www.gravatar.com/avatar/a0ed90e0222a9043a070d2431a539a89?s=128&d=identicon&r=PG	1	0	2	http://sanjayginde.squarespace.com
71	\N	2748037	2014-05-14T02:11:52.550	pkerl	0	74	2014-05-14T02:11:52.550	Atlanta, GA / Boston, MA	https://www.gravatar.com/avatar/75088a57341c179a1de41680d7dbfcda?s=128&d=identicon&r=PG	1	0	1	http://twitter.com/pykerl
72	<p>Deep Learning at Montreal Institute of Learning Algorithms, previously Machine Learning at Adobe Systems, Alumni at IIT Roorkee</p>\n	1429013	2014-05-14T02:15:03.563	Shagun Sodhani	3	75	2018-08-25T12:00:22.700	Canada	\N	687	79	67	https://shagunsodhani.in/
73	<p>Full-Stack Developer (among other things!), based in KL.</p>\n	3792255	2014-05-14T02:16:45.550	Nour	0	76	2018-03-30T06:39:15.737	Cyberjaya Selangor Malaysia	https://i.stack.imgur.com/LkKxK.jpg?s=128&g=1	101	0	1	http://my.linkedin.com/in/nourdt
74	<p>Hi I am Srikar, I think I have been programmed to feel that I need to write this message...</p>\n	49383	2014-05-14T02:31:42.773	Srikar Appalaraju	0	77	2018-08-23T01:05:39.737	\N	https://i.stack.imgur.com/c7KoK.jpg	76	34	8	http://srikar.wordpress.com
75	<p><strong>Python</strong>, <strong>Java</strong>, <strong>Android</strong>, etc. developer living in Charlotte, NC.<br/></p>\n\n<p>I am the author of <a href="https://twitter.com/BountyBot" rel="nofollow noreferrer"><strong>@BountyBot</strong></a>, a Twitter bot that posts new and interesting bounty questions from Stack Overflow. You can <a href="https://github.com/BillCruise/BountyBot" rel="nofollow noreferrer">view the source on GitHub</a>.</p>\n\n<p><strong>My Android apps on Google Play:</strong></p>\n\n<ul>\n<li><a href="https://play.google.com/store/apps/details?id=com.billthelizard.threeofakind&amp;hl=en" rel="nofollow noreferrer"><strong>Three of a Kind</strong></a> - A fun, fast-paced strategy card game. Match any three symbols to win.</li>\n<li><a href="https://play.google.com/store/apps/details?id=com.billthelizard.sketchboard&amp;hl=en" rel="nofollow noreferrer"><strong>Sketchboard</strong></a> - Sketch drawings on your mobile device.</li>\n<li><a href="https://play.google.com/store/apps/details?id=com.billthelizard.serpent&amp;hl=en" rel="nofollow noreferrer"><strong>Serpent</strong></a> - An Android version of the classic mobile game Snake.</li>\n<li><a href="https://play.google.com/store/apps/details?id=com.billthelizard.mathblitz&amp;hl=en" rel="nofollow noreferrer"><strong>Math Blitz!</strong></a> - A fast-paced flashcard game to help students practice their arithmetic skills.</li>\n<li><a href="https://play.google.com/store/apps/details?id=com.billthelizard.linearinterpolator&amp;hl=en" rel="nofollow noreferrer"><strong>Linear Interpolator</strong></a> - Fill in the gaps in your data using linear interpolation.</li>\n<li><a href="https://play.google.com/store/apps/details?id=com.billthelizard.kitchencalculator&amp;hl=en" rel="nofollow noreferrer"><strong>Kitchen Calculator</strong></a> - A simple unit converter for common units used in cooking and homebrewing.</li>\n</ul>\n\n<p>If you use any of these apps, please leave me a rating and any feedback for improvement.</p>\n	970	2014-05-14T02:32:20.483	Bill the Lizard	0	78	2018-06-15T02:38:45.417	Charlotte, NC	\N	101	11	4	http://www.BillTheLizard.com
76	<p><strong>Weekdays</strong>: I work as a frontend developer at Quintype.Inc, learning tonnes of new stuff everyday.</p>\n\n<p><strong>Weekends</strong>: I read books, play games and watch movies.</p>\n	1715779	2014-05-14T02:48:32.760	ThisIzKp	0	79	2015-08-14T16:18:58.107	Bengaluru, Karnataka, India	\N	101	3	2	\N
77	\N	3387953	2014-05-14T03:06:21.010	user2843520	0	80	2018-07-02T17:33:40.723	\N	https://www.gravatar.com/avatar/b29b07aebdb5157aef19c5af0afb5483?s=128&d=identicon&r=PG&f=1	101	0	2	\N
78	\N	4356154	2014-05-14T03:07:51.703	user3554712	0	81	2014-06-12T12:13:31.337	\N	https://www.gravatar.com/avatar/254e1029deddb0b675146bae0b311721?s=128&d=identicon&r=PG&f=1	1	0	1	\N
79	<p>Physics student, Ubuntu user, games fan.</p>\n	514829	2014-05-14T03:16:48.710	Damian Melniczuk	0	82	2018-08-31T10:44:35.320	Wroclaw, Poland	\N	422	128	12	http://data.melniczuk.eu/
80	<p>Laß die Zeit an dir ablaufen wie Wasser</p>\n	1705277	2014-05-14T03:24:28.157	sashkello	0	83	2017-07-10T05:35:56.260	Sydney, Australia	https://www.gravatar.com/avatar/fa4eda8044c652477c07fde37d5699b5?s=128&d=identicon&r=PG	101	0	1	http://secretdruidsociety.org
81	<p>#SOreadytohelp</p>\n\n<p>I'm a computer science undergrad and research assistant at the Federal University of Minas Gerais (UFMG), working with distributed computing and data mining. My favorite programming languages are bash and c++, and I'm amused by unix' wondrous set of magic tools, regular expressions, and <a href="http://twelf.org/wiki/Main_Page" rel="nofollow">Twelf</a>.</p>\n\n<p><sub> <i> "Everything can happen. <br> Everything is possible and probable. <br> Time and space do not exist. <br> On a flimsy framework of reality, <br> the imagination spins, <br> weaving new patterns." </i> <br> Ingmar Bergman -- Fanny and Alexander</sub></p>\n	1822136	2014-05-14T03:30:28.107	Rubens	2	84	2018-07-27T15:48:54.690	Minas Gerais	\N	2557	373	209	http://dcc.ufmg.br/~rubens
82	<p>I am the Architecture Lead for Stack Overflow. My day job consists of being a Developer, Site Reliability Engineer, and DBA. I design and build very fast things in hopes of making life easier for millions of developers.</p>\n\n<p>Blog: <a href="https://nickcraver.com/blog/" rel="nofollow noreferrer">nickcraver.com/blog</a><br>\nTwitter: <a href="https://twitter.com/Nick_Craver" rel="nofollow noreferrer">@Nick_Craver</a><br>\nMy Developer Story: <a href="https://stackoverflow.com/story/ncraver">stackoverflow.com/story/ncraver</a></p>\n\n<p><sub>Disclaimer: I have <em>no idea</em> what I'm talking about, all my answers are guesses!</sub></p>\n	7598	2014-05-14T03:32:28.697	Nick Craver	0	85	2017-03-09T14:40:40.450	Winston-Salem, NC	https://i.stack.imgur.com/nGCYr.jpg?s=128&g=1	101	0	1	https://nickcraver.com/blog/
83	\N	4200366	2014-05-14T03:59:47.193	Derek Hodgson	0	86	2014-05-22T07:24:39.850	\N	\N	1	0	0	\N
84	\N	325297	2014-05-14T04:07:31.467	jonsca	1	87	2017-12-01T03:02:30.977	United States	\N	101	28	3	\N
85	\N	963950	2014-05-14T04:11:57.413	Igor Bobriakov	0	88	2017-05-25T07:45:05.033	\N	https://www.gravatar.com/avatar/f244d83c6c44043718514221586862f7?s=128&d=identicon&r=PG	438	5	22	\N
86	\N	4148647	2014-05-14T04:39:54.360	saq7	0	90	2018-03-13T21:39:37.980	\N	https://www.gravatar.com/avatar/9f041e926e6e951fba1889cdf5556aa5?s=128&d=identicon&r=PG&f=1	382	4	4	\N
87	\N	2418671	2014-05-14T04:50:17.167	prthkms	0	91	2018-08-21T07:07:01.490	\N	https://www.gravatar.com/avatar/61a08a08c58735afe6344d9b4abde02a?s=128&d=identicon&r=PG&f=1	1	2	1	\N
88	\N	2230778	2014-05-14T05:03:02.460	cwharland	2	92	2018-08-23T20:16:57.443	\N	https://www.gravatar.com/avatar/63a734a216422efeab3b81d058f1b7b5?s=128&d=identicon&r=PG	851	13	19	\N
89	<p>A mobile engineer at <a href="http://hipmunk.com" rel="nofollow">Hipmunk</a>!</p>\n\n<p>RE: Android, 140 characters at a time: <a href="https://twitter.com/rskjr" rel="nofollow">my Twitter</a></p>\n\n<p><a href="http://facebook.com/robertkarl" rel="nofollow">Facebook</a></p>\n\n<p><a href="https://github.com/robertkarl" rel="nofollow">Github</a></p>\n	48434	2014-05-14T05:23:16.633	Robert Karl	0	93	2014-05-14T05:23:16.633	San Francisco	\N	101	0	1	http://hipmunk.com
90	<p>The Big Picture.</p>\n	112206	2014-05-14T05:23:51.243	Yeti	0	94	2017-07-30T07:27:01.440	Budapest, Hungary	https://www.gravatar.com/avatar/995e1762e0815a7d3f6acf3bb88b8f9c?s=128&d=identicon&r=PG	1	0	1	http://primeranks.net
91	<p>Accidental coder with a cynical view of code and life in general but I believe that shared knowledge empowers the masses. <em>'Hell is other people's code'.</em></p>\n\n<h1>SOreadytohelp</h1>\n	735810	2014-05-14T05:26:55.373	EdChum	2	95	2018-07-24T12:43:05.463	Berkshire, United Kingdom	\N	215	19	11	\N
92	\N	361515	2014-05-14T05:27:09.957	ePezhman	1	96	2018-06-21T14:10:36.920	\N	https://i.stack.imgur.com/MV9ND.jpg?s=128&g=1	188	0	8	\N
93	<p>Data analytics team lead, currently working in online gaming industry (Wargaming).</p>\n\n<p>Linkedin: <a href="https://www.linkedin.com/in/shyroki" rel="nofollow noreferrer">https://www.linkedin.com/in/shyroki</a></p>\n	241216	2014-05-14T05:32:45.010	IharS	12	97	2018-08-28T16:02:27.397	Minsk, Belarus	https://www.gravatar.com/avatar/68acdc33e278ab19debf230662a7ad96?s=128&d=identicon&r=PG	2326	308	143	https://www.linkedin.com/in/shyroki
94	<p>Knowledge in:</p>\n\n<ul>\n<li>Java/J2EE</li>\n<li>Web Services </li>\n<li>XML/XSLT/XSD </li>\n<li>JavaScript </li>\n<li>PL/SQL</li>\n<li>Oracle ADF</li>\n</ul>\n	1038747	2014-05-14T05:35:55.000	Fr_nkenstien	0	98	2018-04-02T14:19:40.467	India	https://i.stack.imgur.com/sjIdN.jpg	101	0	1	https://techutils.in
95	<p>Dotnet coder (C#, WPF) and browsergame coder (PHP)</p>\n	2242935	2014-05-14T05:44:39.163	Xaruth	0	99	2014-05-14T09:24:43.200	France	https://i.stack.imgur.com/PoLl7.jpg?s=128&g=1	101	0	1	\N
96	<p>I am a moderator on Super User\n<br/><br/>\nI work on medical image processing at <a href="https://www.clinicalgraphics.com" rel="nofollow noreferrer">Clinical Graphics</a>.\n<br/><br/>\nYou can follow me on Twitter: <a href="http://twitter.com/#!/ivoflipse5" rel="nofollow noreferrer">ivoflipse5</a></p>\n\n<p><a href="http://stackoverflow.com/users/77595/ivo-flipse">\n<img src="https://stackoverflow.com/users/flair/77595.png" width="208" height="58" alt="profile for Ivo Flipse at Stack Overflow, Q&amp;A for professional and enthusiast programmers" title="profile for Ivo Flipse at Stack Overflow, Q&amp;A for professional and enthusiast programmers">\n</a></p>\n	28900	2014-05-14T06:06:21.970	Ivo Flipse	0	100	2018-01-10T11:14:05.690	Bemmel, Netherlands	\N	101	38	1	http://www.flipserd.com/blog
97	<p>Teaching statistics to biology students and working as data analyst.</p>\n\n<p>My professional information on <a href="http://www.linkedin.com/pub/didzis-elferts/67/624/aa8" rel="nofollow">LinkedIn</a></p>\n	2084934	2014-05-14T06:15:18.753	Didzis Elferts	1	101	2016-04-29T12:52:57.307	Latvia	https://i.stack.imgur.com/yGHJ7.jpg?s=128&g=1	101	1	2	http://delferts.github.io
98	\N	192005	2014-05-14T06:24:54.137	Bryan	0	102	2015-05-21T19:09:02.617	\N	https://www.gravatar.com/avatar/1d9bb862b2c539accf7464f2e8346b7e?s=128&d=identicon&r=PG	121	0	2	\N
99	<p>Web (angular, node, yeoman..etc), JAVA and mobile developer (Android, cordova and iOS).\nPassionate of ski &amp; télémark, paragliding and moto. Love to run on mountains, high tech stuffs, sport and having fun ;)</p>\n\n<p>Currently working around Genova.</p>\n	2454969	2014-05-14T06:36:29.273	tanou	0	104	2017-08-09T12:52:56.840	Annecy, France	\N	259	2	5	https://github.com/tanou73
\.


--
-- Name: ix_posts_index; Type: INDEX; Schema: public; Owner: xavier
--

CREATE INDEX ix_posts_index ON public.posts USING btree (index);


--
-- Name: ix_tags_index; Type: INDEX; Schema: public; Owner: xavier
--

CREATE INDEX ix_tags_index ON public.tags USING btree (index);


--
-- Name: ix_users_index; Type: INDEX; Schema: public; Owner: xavier
--

CREATE INDEX ix_users_index ON public.users USING btree (index);


--
-- PostgreSQL database dump complete
--

